{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a string s = 'colorless'. Write a Python statement that changes this to\n",
    "“colourless” using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'colorless'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:4] + 'u' + s[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the slice notation to remove morphological endings on words. For\n",
    "example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice\n",
    "notation to remove the affixes from these words (we’ve inserted a hyphen to indicate\n",
    "the affix boundary, but omit this from your strings): dish-es, run-ning, nationality,\n",
    "un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish-es'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'dishes'\n",
    "w[:-2] + '-' + w[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run-ning'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'running'\n",
    "w[:3] + '-'  + w[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nation-ality'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'nationality'\n",
    "w[:6] + '-' + w[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un-do'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'undo'\n",
    "w[:2] + '-' + w[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pre-heat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'preheat'\n",
    "w[:3] + '-' + w[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how we can generate an IndexError by indexing beyond the end of a\n",
    "string. Is it possible to construct an index that goes too far to the left, before the\n",
    "start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 c\n",
      "2 b\n",
      "3 a\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5e12b44396f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'abc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# w[0], w[-1], w[-2], w[-3], w[-4]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "w = 'abc'\n",
    "for i in range(len(w) + 2):\n",
    "    print(i, w[-i])\n",
    "# w[0], w[-1], w[-2], w[-3], w[-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, no. The minimum index value is -len(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a “step” size for the slice. The following returns every second\n",
    "character within the slice: monty[6:11:2]. It also works in the reverse direction:\n",
    "monty[10:5:-2]. Try these for yourself, and then experiment with different step\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lowr'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[2:9:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!r l'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[-1:0:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you ask the interpreter to evaluate monty[::-1]? Explain why\n",
    "this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!dlrow olleH'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse string. Moving from the start to end of the string with the negative step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions:<div>\n",
    "a. [a-zA-Z]+<div>\n",
    "b. [A-Z][a-z]*<div>\n",
    "c. p[aeiou]{,2}t<div>\n",
    "d. \\d+(\\.\\d+)?<div>\n",
    "e. ([^aeiou][aeiou][^aeiou])*<div>\n",
    "f. \\w+|[^\\w\\s]+<div>\n",
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. any combination of one or more letters in lower/upper case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. any pair of letters, where the first letter is in upper case and the second is in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. any 3-4-letter word that starts with 'p', ends with 't', and contains inside 1-2 vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. any number (may have leading zeroes), that has or hasn't a floating part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. 3-char combinations that starts with non-vowel chars followed by vowel char, and ends with non-vowwel char again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. any alphanumeric sequence or any sequence that doesn't contain alphanumeric or space chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:<div>\n",
    "a. A single determiner (assume that a, an, and the are the only determiners)<div>\n",
    "b. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{A} single determiner (assume that {a}, {an}, and {the} are {the} only determiners)\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(\n",
    "    regexp='\\\\b([aA]|[aA]n|[tT]he)\\\\b',\n",
    "    string='A single determiner (assume that a, an, and the are the only determiners)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An arithmetic expression using integers, addition, and multiplication, such as {2*3+8}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(\n",
    "    regexp='\\d([*+]\\d)+',\n",
    "    string='An arithmetic expression using integers, addition, and multiplication, such as 2*3+8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a utility function that takes a URL as its argument, and returns the contents\n",
    "of the URL, with all HTML markup removed. Use urllib.urlopen to access the contents of the URL, e.g.:\n",
    "raw_contents = urllib.urlopen('http://www.nltk.org/').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('http://www.nltk.org/').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = BeautifulSoup(html, \"lxml\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nNatural Language Toolkit — NLTK 3.4.4 documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK 3.4.4 documentation\\n\\nnext |\\n          modules |\\n          index\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNatural Language Toolkit¶\\nNLTK is a leading platform for building Python programs to work with human language data.\\nIt provides easy-to-use interfaces to over 50 corpora and lexical\\nresources such as WordNet,\\nalong with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\\nwrappers for industrial-strength NLP libraries,\\nand an active discussion forum.\\nThanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\\nNLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\\nNLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\\nNLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\\nand “an amazing library to play with natural language.”\\nNatural Language Processing with Python provides a practical\\nintroduction to programming for language processing.\\nWritten by the creators of NLTK, it guides the reader through the fundamentals\\nof writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\\nand more.\\nThe online version of the book has been been updated for Python 3 and NLTK 3.\\n(The original Python 2 version is still available at http://nltk.org/book_1ed.)\\n\\nSome simple things you can do with NLTK¶\\nTokenize and tag some text:\\n>>> import nltk\\n>>> sentence = \"\"\"At eight o\\'clock on Thursday morning\\n... Arthur didn\\'t feel very good.\"\"\"\\n>>> tokens = nltk.word_tokenize(sentence)\\n>>> tokens\\n[\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\',\\n\\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\']\\n>>> tagged = nltk.pos_tag(tokens)\\n>>> tagged[0:6]\\n[(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'),\\n(\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')]\\n\\n\\nIdentify named entities:\\n>>> entities = nltk.chunk.ne_chunk(tagged)\\n>>> entities\\nTree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'),\\n           (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'),\\n       Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]),\\n           (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'),\\n           (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')])\\n\\n\\nDisplay a parse tree:\\n>>> from nltk.corpus import treebank\\n>>> t = treebank.parsed_sents(\\'wsj_0001.mrg\\')[0]\\n>>> t.draw()\\n\\n\\n\\nNB. If you publish work that uses NLTK, please cite the NLTK book as\\nfollows:\\n\\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\\n\\n\\n\\nNext Steps¶\\n\\nsign up for release announcements\\njoin in the discussion\\n\\n\\n\\n\\nContents¶\\n\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n\\nIndex\\nModule Index\\nSearch Page\\n\\n\\n\\n\\n\\n\\n\\nTable of Contents\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnext |\\n            modules |\\n            index\\n\\n\\n\\nShow Source\\n\\n\\n\\n\\n        © Copyright 2019, NLTK Project.\\n      Last updated on Jul 04, 2019.\\n      Created using Sphinx 2.1.2.\\n    \\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save some text into a file corpus.txt. Define a function load(f) that reads from\n",
    "the file named in its sole argument, and returns a string containing the text of the\n",
    "file.<div>\n",
    "a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various\n",
    "kinds of punctuation in this text. Use one multiline regular expression inline\n",
    "comments, using the verbose flag (?x).<div>\n",
    "b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following\n",
    "kinds of expressions: monetary amounts; dates; names of people and\n",
    "organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename, encoding='utf8'):\n",
    "    with open(filename, encoding=encoding) as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Save some text into a file corpus.txt. Define a function load(f) that reads €100 from the file named in its sole argument, and returns a string containing the text of the file.\\n\\n$56.12 ₽56.56\\n\\na. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multiline regular expression inline comments, using the verbose flag (?x).\\nb. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expressions: monetary amounts; dates; names of people and organizations    !\"#%&\\'()*+,-./:;<=>?@[]^_`{|}~\\\\ffffff\\n\\n2019-04-15\\n\\nName Surname\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = load('ch3_ex09.txt')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '(', ')', ',', '.', '.', '.', '.', '.', '_', '(', ')', '.', ',', '(', '?', ')', '.', '.', '.', '_', '(', ')', ':', ';', ';', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '\\\\', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(text, \"\"\"[!\"#%&'()*+,-./:;<=>?@\\[\\]^_`{|}~\\\\\\\\]\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"\"\"(?x)\n",
    "    [$€₽]\\d+(?:\\.\\d+)?\n",
    "  | \\d{4}-\\d{2}-\\d{2}\n",
    "  | [A-Z]\\w+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Save', 'Define', '€100', '$56.12', '₽56.56', 'Use', 'Use', 'Use', '2019-04-15', 'Name', 'Surname']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(text, pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3),\n",
       " ('dog', 3),\n",
       " ('gave', 4),\n",
       " ('John', 4),\n",
       " ('the', 3),\n",
       " ('newspaper', 9)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = [(w, len(w)) for w in sent]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a string raw containing a sentence of your own choosing. Now, split raw\n",
    "on some character other than space, such as 's'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"Define a string raw containing a sentence of your own choosing. Now, split raw\n",
    "on some character other than space, such as 's'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Define a ',\n",
       " 'tring raw containing a ',\n",
       " 'entence of your own choo',\n",
       " 'ing. Now, ',\n",
       " 'plit raw\\non ',\n",
       " 'ome character other than ',\n",
       " 'pace, ',\n",
       " 'uch a',\n",
       " \" '\",\n",
       " \"'\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.split('s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "e\n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "r\n",
      "a\n",
      "w\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "e\n",
      "n\n",
      "t\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "c\n",
      "h\n",
      "o\n",
      "o\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n",
      " \n",
      "N\n",
      "o\n",
      "w\n",
      ",\n",
      " \n",
      "s\n",
      "p\n",
      "l\n",
      "i\n",
      "t\n",
      " \n",
      "r\n",
      "a\n",
      "w\n",
      "\n",
      "\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "'\n",
      "s\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "for c in raw:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between calling split on a string with no argument and\n",
    "one with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What\n",
    "happens when the string being split contains tab characters, consecutive space\n",
    "characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to\n",
    "enter a tab character.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If sep is not specified or is None, any whitespace string (\\s) is a separator and empty strings are removed from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd', 'sad', 'asdasds']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asd sad\\n\\t\\t\\t\\t\\r   asdasds'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd', 'sad\\n\\t\\t\\t\\t\\r', '', '', 'asdasds']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asd sad\\n\\t\\t\\t\\t\\r   asdasds'.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable words containing a list of words. Experiment with\n",
    "words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Create', 'a', 'variable', 'words', 'containing', 'a', 'list', 'of', 'words']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize('Create a variable words containing a list of words')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Create', 'a', 'a', 'containing', 'list', 'of', 'variable', 'words', 'words']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sort()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Create', 'a', 'a', 'containing', 'list', 'of', 'variable', 'words', 'words']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to https://stackoverflow.com/questions/22442378/what-is-the-difference-between-sortedlist-vs-list-sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>sorted() returns a new sorted list</b>, leaving the original list unaffected. <b>list.sort() sorts the list in-place</b>, mutating the list indices, and returns None (like all in-place operations).\n",
    "\n",
    "sorted() works on any iterable, not just lists. Strings, tuples, dictionaries (you'll get the keys), generators, etc., returning a list containing all elements, sorted.\n",
    "\n",
    "Use list.sort() when you want to mutate the list, sorted() when you want a new sorted object back. Use sorted() when you want to sort something that is an iterable, not a list yet.\n",
    "\n",
    "For lists, <b>list.sort() is faster than sorted()</b> because it doesn't have to create a copy. For any other iterable, you have no choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the difference between strings and integers by typing the following at a\n",
    "Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers\n",
    "using int(\"3\") and str(3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3333333', 21)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 7,  3 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, '3')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"3\"),  str(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we asked you to use a text editor to create a file called test.py, containing the single line monty = 'Monty Python'. If you haven’t already done this (or can’t find the file), go ahead and do it now. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):<div>\n",
    "<b>from test import msg</b><div>\n",
    "<b>msg</b><div>\n",
    "This time, Python should return with a value. You can also try import test, in which case Python should be able to evaluate the expression test.monty at the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-d4cc90107335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmonty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'monty' is not defined"
     ]
    }
   ],
   "source": [
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch3_ex16 import monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ch3_ex16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch3_ex16.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when the formatting strings %6s and %-6s are used to display\n",
    "strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aaaaaaa', '    bb')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%6s' % 'aaaaaaa', '%6s' % 'bb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bbbbbbb', 'bb    ')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%-6s' % 'bbbbbbb', '%-6s' % 'bb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word\n",
    "types that occur. (wh-words in English are used in questions, relative clauses, and\n",
    "exclamations: who, which, what, and so on.) Print them in order. Are any words\n",
    "duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = gutenberg.raw('carroll-alice.txt')\n",
    "words = [w.lower() for w in nltk.word_tokenize(raw_text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_word_types = sorted(\n",
    "    set(w for w in words if w.startswith('wh'))\n",
    "    & set(set(stopwords.words('english') + ['whose']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'when', 'where', 'which', 'while', 'who', 'whom', 'whose', 'why']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_word_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file consisting of words and (made up) frequencies, where each line\n",
    "consists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read\n",
    "the file into a Python list using open(filename).readlines(). Next, break each line\n",
    "into its two fields using split(), and convert the number into an integer using\n",
    "int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['x', 53], ['y', 156], ['z', 567]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "with open('ch3_ex19.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        w, freq = line.split()\n",
    "        result.append([w, int(freq)])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to access a favorite web page and extract some text from it. For\n",
    "example, access a weather site and extract the forecast top temperature for your\n",
    "town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.tut.by/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$2.0488'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dollar_rate = (\n",
    "    content\n",
    "    .select_one('a[href=\"https://finance.tut.by/kurs/\"]')\n",
    "    .get_text(strip=True)\n",
    ")\n",
    "dollar_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function unknown() that takes a URL as its argument, and returns a list\n",
    "of unknown words that occur on that web page. In order to do this, extract all\n",
    "substrings consisting of lowercase letters (using re.findall()) and remove any\n",
    "items from this set that occur in the Words Corpus (nltk.corpus.words). Try to\n",
    "categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown(url, regex='[a-z]+'):\n",
    "    response = requests.get(url)\n",
    "    text = BeautifulSoup(response.text, \"lxml\").get_text()\n",
    "    words_set = set(words.words())\n",
    "    return sorted(set([\n",
    "        s for s in re.findall(regex, text) \n",
    "        if s not in words_set\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words = unknown(\"https://en.wikipedia.org/wiki/Plain_text\")\n",
    "len(unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abel',\n",
       " 'ac',\n",
       " 'accented',\n",
       " 'acintosh',\n",
       " 'ackend',\n",
       " 'adding',\n",
       " 'adges',\n",
       " 'adget',\n",
       " 'ahasa',\n",
       " 'ain',\n",
       " 'ais',\n",
       " 'allbacks',\n",
       " 'allocated',\n",
       " 'allows',\n",
       " 'ames',\n",
       " 'amespace',\n",
       " 'amespaces',\n",
       " 'andom',\n",
       " 'anguage',\n",
       " 'anguages',\n",
       " 'anonical',\n",
       " 'anuary',\n",
       " 'applications',\n",
       " 'arametric',\n",
       " 'arget',\n",
       " 'argued',\n",
       " 'ariant',\n",
       " 'ariants',\n",
       " 'arkup',\n",
       " 'articles',\n",
       " 'assigning',\n",
       " 'assigns',\n",
       " 'ata',\n",
       " 'atal',\n",
       " 'ategories',\n",
       " 'atin',\n",
       " 'ational',\n",
       " 'autostart',\n",
       " 'av',\n",
       " 'avoided',\n",
       " 'became',\n",
       " 'bfloat',\n",
       " 'bi',\n",
       " 'bignum',\n",
       " 'bitmapped',\n",
       " 'bits',\n",
       " 'bject',\n",
       " 'books',\n",
       " 'breaks',\n",
       " 'browsers',\n",
       " 'bstract',\n",
       " 'bytes',\n",
       " 'cachereport',\n",
       " 'categories',\n",
       " 'ccording',\n",
       " 'centralauth',\n",
       " 'centralautologin',\n",
       " 'challenged',\n",
       " 'changes',\n",
       " 'characters',\n",
       " 'charinsert',\n",
       " 'charset',\n",
       " 'checksum',\n",
       " 'chema',\n",
       " 'citations',\n",
       " 'cleartext',\n",
       " 'codes',\n",
       " 'commands',\n",
       " 'compactlinks',\n",
       " 'companies',\n",
       " 'completed',\n",
       " 'computers',\n",
       " 'computing',\n",
       " 'concerns',\n",
       " 'config',\n",
       " 'conflicts',\n",
       " 'considers',\n",
       " 'consisting',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'conventions',\n",
       " 'countries',\n",
       " 'covers',\n",
       " 'cputime',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'cs',\n",
       " 'csrf',\n",
       " 'ct',\n",
       " 'ction',\n",
       " 'ctober',\n",
       " 'ctuple',\n",
       " 'dashes',\n",
       " 'ddress',\n",
       " 'defines',\n",
       " 'desktop',\n",
       " 'developed',\n",
       " 'devices',\n",
       " 'died',\n",
       " 'digits',\n",
       " 'dingbats',\n",
       " 'disambiguation',\n",
       " 'ditable',\n",
       " 'ditor',\n",
       " 'dmy',\n",
       " 'documents',\n",
       " 'doesn',\n",
       " 'doi',\n",
       " 'drawings',\n",
       " 'ead',\n",
       " 'earch',\n",
       " 'eatured',\n",
       " 'eb',\n",
       " 'ebruary',\n",
       " 'ec',\n",
       " 'ecember',\n",
       " 'ecent',\n",
       " 'ecimal',\n",
       " 'ecord',\n",
       " 'ecursive',\n",
       " 'ederlands',\n",
       " 'edia',\n",
       " 'edirect',\n",
       " 'edited',\n",
       " 'editors',\n",
       " 'educed',\n",
       " 'ee',\n",
       " 'efault',\n",
       " 'eference',\n",
       " 'eferences',\n",
       " 'efforts',\n",
       " 'efimprove',\n",
       " 'efinement',\n",
       " 'eflist',\n",
       " 'efore',\n",
       " 'elatedtopics',\n",
       " 'elayu',\n",
       " 'elevant',\n",
       " 'elp',\n",
       " 'email',\n",
       " 'emaphore',\n",
       " 'emoji',\n",
       " 'emplate',\n",
       " 'enables',\n",
       " 'encoded',\n",
       " 'encoding',\n",
       " 'encodings',\n",
       " 'endianness',\n",
       " 'enear',\n",
       " 'eneral',\n",
       " 'eneric',\n",
       " 'enhancements',\n",
       " 'entityaccesscount',\n",
       " 'entral',\n",
       " 'ep',\n",
       " 'eparator',\n",
       " 'ependent',\n",
       " 'eport',\n",
       " 'eptember',\n",
       " 'equest',\n",
       " 'erhaps',\n",
       " 'erman',\n",
       " 'ermanent',\n",
       " 'erms',\n",
       " 'ersonal',\n",
       " 'escriptions',\n",
       " 'esponse',\n",
       " 'estriction',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'etrieved',\n",
       " 'eutsch',\n",
       " 'evelopers',\n",
       " 'events',\n",
       " 'evision',\n",
       " 'expansiondepth',\n",
       " 'expensivefunctioncount',\n",
       " 'ext',\n",
       " 'failed',\n",
       " 'figuring',\n",
       " 'files',\n",
       " 'fonts',\n",
       " 'formats',\n",
       " 'formatted',\n",
       " 'formatting',\n",
       " 'forms',\n",
       " 'fter',\n",
       " 'googpub',\n",
       " 'hapter',\n",
       " 'haracter',\n",
       " 'has',\n",
       " 'hese',\n",
       " 'homas',\n",
       " 'homisticus',\n",
       " 'hor',\n",
       " 'hort',\n",
       " 'html',\n",
       " 'http',\n",
       " 'https',\n",
       " 'humans',\n",
       " 'hus',\n",
       " 'hypertext',\n",
       " 'hyphens',\n",
       " 'icense',\n",
       " 'idden',\n",
       " 'identified',\n",
       " 'ideographs',\n",
       " 'iew',\n",
       " 'iewer',\n",
       " 'iews',\n",
       " 'igit',\n",
       " 'ignoring',\n",
       " 'ikibase',\n",
       " 'ikidata',\n",
       " 'ikimedia',\n",
       " 'ikipedia',\n",
       " 'ikisource',\n",
       " 'iles',\n",
       " 'images',\n",
       " 'ime',\n",
       " 'iming',\n",
       " 'imple',\n",
       " 'inary',\n",
       " 'including',\n",
       " 'incompatibilities',\n",
       " 'ind',\n",
       " 'individuals',\n",
       " 'indows',\n",
       " 'ine',\n",
       " 'inifloat',\n",
       " 'init',\n",
       " 'instructions',\n",
       " 'integers',\n",
       " 'interpreted',\n",
       " 'ir',\n",
       " 'isclaimers',\n",
       " 'isplay',\n",
       " 'isual',\n",
       " 'itation',\n",
       " 'ite',\n",
       " 'ith',\n",
       " 'itle',\n",
       " 'ix',\n",
       " 'ixed',\n",
       " 'ixon',\n",
       " 'js',\n",
       " 'json',\n",
       " 'keying',\n",
       " 'laintext',\n",
       " 'languages',\n",
       " 'lement',\n",
       " 'letters',\n",
       " 'lgebraic',\n",
       " 'ligatures',\n",
       " 'limitreport',\n",
       " 'll',\n",
       " 'llen',\n",
       " 'loating',\n",
       " 'logo',\n",
       " 'loosest',\n",
       " 'lthough',\n",
       " 'ltr',\n",
       " 'machines',\n",
       " 'mainspace',\n",
       " 'matters',\n",
       " 'mbox',\n",
       " 'mediawiki',\n",
       " 'memusage',\n",
       " 'metaclass',\n",
       " 'metaobject',\n",
       " 'mmv',\n",
       " 'mw',\n",
       " 'nabled',\n",
       " 'nc',\n",
       " 'ncoding',\n",
       " 'ndex',\n",
       " 'ndianness',\n",
       " 'ndonesia',\n",
       " 'ndrew',\n",
       " 'nductive',\n",
       " 'needed',\n",
       " 'newline',\n",
       " 'newspapers',\n",
       " 'ng',\n",
       " 'ngland',\n",
       " 'nglish',\n",
       " 'nicode',\n",
       " 'nimals',\n",
       " 'ninterpreted',\n",
       " 'nion',\n",
       " 'nojs',\n",
       " 'nomin',\n",
       " 'noscript',\n",
       " 'nsourced',\n",
       " 'nteger',\n",
       " 'nteraction',\n",
       " 'nternational',\n",
       " 'nterval',\n",
       " 'ntity',\n",
       " 'numbers',\n",
       " 'numerated',\n",
       " 'oader',\n",
       " 'oberto',\n",
       " 'obile',\n",
       " 'objects',\n",
       " 'odified',\n",
       " 'og',\n",
       " 'ogging',\n",
       " 'oid',\n",
       " 'ointer',\n",
       " 'oken',\n",
       " 'ol',\n",
       " 'oldid',\n",
       " 'olicy',\n",
       " 'ollection',\n",
       " 'omain',\n",
       " 'oman',\n",
       " 'ommons',\n",
       " 'ommunications',\n",
       " 'ommunity',\n",
       " 'omplex',\n",
       " 'omposite',\n",
       " 'omputers',\n",
       " 'onate',\n",
       " 'ones',\n",
       " 'onflicts',\n",
       " 'ong',\n",
       " 'onsortium',\n",
       " 'ontact',\n",
       " 'ontent',\n",
       " 'ontents',\n",
       " 'onth',\n",
       " 'ontributions',\n",
       " 'ontributors',\n",
       " 'ontrol',\n",
       " 'ookie',\n",
       " 'oolbar',\n",
       " 'oolean',\n",
       " 'ools',\n",
       " 'ooltips',\n",
       " 'oombs',\n",
       " 'op',\n",
       " 'opened',\n",
       " 'opted',\n",
       " 'options',\n",
       " 'opup',\n",
       " 'opups',\n",
       " 'ord',\n",
       " 'org',\n",
       " 'organisations',\n",
       " 'ormat',\n",
       " 'orpus',\n",
       " 'orsk',\n",
       " 'ortugu',\n",
       " 'ost',\n",
       " 'ostname',\n",
       " 'ot',\n",
       " 'others',\n",
       " 'otice',\n",
       " 'ottom',\n",
       " 'ouble',\n",
       " 'oundation',\n",
       " 'ource',\n",
       " 'ov',\n",
       " 'ove',\n",
       " 'ovember',\n",
       " 'owered',\n",
       " 'ownload',\n",
       " 'oyal',\n",
       " 'pages',\n",
       " 'panish',\n",
       " 'paque',\n",
       " 'paragraphs',\n",
       " 'parts',\n",
       " 'pecial',\n",
       " 'permitting',\n",
       " 'php',\n",
       " 'pload',\n",
       " 'png',\n",
       " 'popups',\n",
       " 'portions',\n",
       " 'positions',\n",
       " 'postexpandincludesize',\n",
       " 'ppgeneratednodes',\n",
       " 'ppvisitednodes',\n",
       " 'pr',\n",
       " 'pril',\n",
       " 'printers',\n",
       " 'problems',\n",
       " 'processing',\n",
       " 'programming',\n",
       " 'programs',\n",
       " 'projects',\n",
       " 'properties',\n",
       " 'ption',\n",
       " 'px',\n",
       " 'quicksurveys',\n",
       " 'quotes',\n",
       " 'ragmatic',\n",
       " 'rames',\n",
       " 'ransform',\n",
       " 'rbitrary',\n",
       " 'readers',\n",
       " 'reassigning',\n",
       " 'reate',\n",
       " 'reative',\n",
       " 'references',\n",
       " 'remaining',\n",
       " 'rench',\n",
       " 'representations',\n",
       " 'represented',\n",
       " 'reserves',\n",
       " 'reviews',\n",
       " 'rganisation',\n",
       " 'rganization',\n",
       " 'rimitive',\n",
       " 'rint',\n",
       " 'rintable',\n",
       " 'rivacy',\n",
       " 'robably',\n",
       " 'roduct',\n",
       " 'rogrammer',\n",
       " 'roject',\n",
       " 'rom',\n",
       " 'rooks',\n",
       " 'rotocol',\n",
       " 'roups',\n",
       " 'rown',\n",
       " 'rowser',\n",
       " 'rray',\n",
       " 'rticle',\n",
       " 'rticles',\n",
       " 'rules',\n",
       " 'ryte',\n",
       " 'scribunto',\n",
       " 'sections',\n",
       " 'selectors',\n",
       " 'sets',\n",
       " 'settings',\n",
       " 'shared',\n",
       " 'signedness',\n",
       " 'skins',\n",
       " 'sources',\n",
       " 'spaces',\n",
       " 'speranto',\n",
       " 'ss',\n",
       " 'ssociative',\n",
       " 'startup',\n",
       " 'statements',\n",
       " 'stating',\n",
       " 'stored',\n",
       " 'storing',\n",
       " 'streams',\n",
       " 'structures',\n",
       " 'styled',\n",
       " 'styles',\n",
       " 'subsets',\n",
       " 'svg',\n",
       " 'symbols',\n",
       " 'systems',\n",
       " 'tabs',\n",
       " 'tagline',\n",
       " 'taliano',\n",
       " 'tandard',\n",
       " 'tandardisation',\n",
       " 'tem',\n",
       " 'templateargumentsize',\n",
       " 'tep',\n",
       " 'terminated',\n",
       " 'terms',\n",
       " 'teven',\n",
       " 'texts',\n",
       " 'tffind',\n",
       " 'ther',\n",
       " 'things',\n",
       " 'timestamp',\n",
       " 'timeusage',\n",
       " 'timingprofile',\n",
       " 'tina',\n",
       " 'toc',\n",
       " 'tokens',\n",
       " 'toolbar',\n",
       " 'tools',\n",
       " 'trademark',\n",
       " 'transientcontent',\n",
       " 'translating',\n",
       " 'tream',\n",
       " 'tring',\n",
       " 'truct',\n",
       " 'tructure',\n",
       " 'ttempt',\n",
       " 'ttl',\n",
       " 'ttribution',\n",
       " 'txt',\n",
       " 'types',\n",
       " 'uadruple',\n",
       " 'ublish',\n",
       " 'ublished',\n",
       " 'ubmit',\n",
       " 'ubtyping',\n",
       " 'uery',\n",
       " 'uggest',\n",
       " 'ugust',\n",
       " 'ul',\n",
       " 'uls',\n",
       " 'uly',\n",
       " 'uman',\n",
       " 'umeric',\n",
       " 'une',\n",
       " 'units',\n",
       " 'unsourced',\n",
       " 'unt',\n",
       " 'upload',\n",
       " 'url',\n",
       " 'uropean',\n",
       " 'urrent',\n",
       " 'usa',\n",
       " 'usages',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'ussian',\n",
       " 'uth',\n",
       " 'utilities',\n",
       " 'utonym',\n",
       " 'utton',\n",
       " 'ux',\n",
       " 'values',\n",
       " 'variations',\n",
       " 'venska',\n",
       " 'vents',\n",
       " 'versample',\n",
       " 'vte',\n",
       " 'walltime',\n",
       " 'watchlist',\n",
       " 'wg',\n",
       " 'wiki',\n",
       " 'wikibase',\n",
       " 'wikidata',\n",
       " 'wikimedia',\n",
       " 'wikipedia',\n",
       " 'wikitext',\n",
       " 'wl',\n",
       " 'wmf',\n",
       " 'ws',\n",
       " 'www',\n",
       " 'xception',\n",
       " 'xtended',\n",
       " 'xterm',\n",
       " 'years',\n",
       " 'ynx',\n",
       " 'ype',\n",
       " 'yte']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many plural nouns and forms of the same verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the results of processing the URL http://news.bbc.co.uk/ using the regular\n",
    "expressions suggested above. You will see that there is still a fair amount of\n",
    "non-textual data there, particularly JavaScript commands. You may also find that\n",
    "sentence breaks have not been properly preserved. Define further regular expressions\n",
    "that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_bbc = unknown(\"http://news.bbc.co.uk/\", regex='\\\\b[a-z]+\\\\b')\n",
    "len(unknown_bbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abs',\n",
       " 'accuses',\n",
       " 'adchoices',\n",
       " 'adding',\n",
       " 'additions',\n",
       " 'ads',\n",
       " 'adverts',\n",
       " 'alsos',\n",
       " 'amp',\n",
       " 'antialiased',\n",
       " 'ap',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apps',\n",
       " 'arguments',\n",
       " 'articles',\n",
       " 'arts',\n",
       " 'async',\n",
       " 'attributes',\n",
       " 'autocapitalize',\n",
       " 'autocomplete',\n",
       " 'autocorrect',\n",
       " 'auxclick',\n",
       " 'balatarin',\n",
       " 'bbc',\n",
       " 'bbccookies',\n",
       " 'bbcdotcom',\n",
       " 'bbci',\n",
       " 'bbcpage',\n",
       " 'bbcprivacy',\n",
       " 'bbcredirection',\n",
       " 'bbcthree',\n",
       " 'bbcuser',\n",
       " 'bbcworldwide',\n",
       " 'biowaste',\n",
       " 'bitesize',\n",
       " 'blog',\n",
       " 'bossa',\n",
       " 'bramstein',\n",
       " 'branding',\n",
       " 'browsers',\n",
       " 'btn',\n",
       " 'bubbles',\n",
       " 'bugs',\n",
       " 'bundles',\n",
       " 'calc',\n",
       " 'callback',\n",
       " 'called',\n",
       " 'cbbc',\n",
       " 'cbeebies',\n",
       " 'cc',\n",
       " 'ccauds',\n",
       " 'cdn',\n",
       " 'cf',\n",
       " 'challenging',\n",
       " 'charset',\n",
       " 'chartbeat',\n",
       " 'checksum',\n",
       " 'children',\n",
       " 'choices',\n",
       " 'claims',\n",
       " 'closeable',\n",
       " 'cmd',\n",
       " 'co',\n",
       " 'com',\n",
       " 'comments',\n",
       " 'comp',\n",
       " 'conditions',\n",
       " 'config',\n",
       " 'configurable',\n",
       " 'const',\n",
       " 'contexts',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'cps',\n",
       " 'criticised',\n",
       " 'crwdcntrl',\n",
       " 'css',\n",
       " 'cta',\n",
       " 'ctrl',\n",
       " 'cy',\n",
       " 'dcdcdc',\n",
       " 'debug',\n",
       " 'declarations',\n",
       " 'defied',\n",
       " 'degrees',\n",
       " 'deps',\n",
       " 'detectview',\n",
       " 'determining',\n",
       " 'dialog',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'dist',\n",
       " 'dns',\n",
       " 'docks',\n",
       " 'docs',\n",
       " 'douban',\n",
       " 'doubles',\n",
       " 'dp',\n",
       " 'echoclient',\n",
       " 'edr',\n",
       " 'effectivemeasure',\n",
       " 'ejected',\n",
       " 'elem',\n",
       " 'elements',\n",
       " 'eles',\n",
       " 'embraced',\n",
       " 'emp',\n",
       " 'endif',\n",
       " 'env',\n",
       " 'envs',\n",
       " 'eol',\n",
       " 'eslint',\n",
       " 'evt',\n",
       " 'exec',\n",
       " 'executing',\n",
       " 'expires',\n",
       " 'expressions',\n",
       " 'facebook',\n",
       " 'features',\n",
       " 'feeds',\n",
       " 'fff',\n",
       " 'files',\n",
       " 'finals',\n",
       " 'flexbox',\n",
       " 'flipboard',\n",
       " 'footballers',\n",
       " 'footerbutton',\n",
       " 'frameworks',\n",
       " 'friends',\n",
       " 'gb',\n",
       " 'gd',\n",
       " 'geolocation',\n",
       " 'getcount',\n",
       " 'gets',\n",
       " 'github',\n",
       " 'google',\n",
       " 'googleplus',\n",
       " 'googletag',\n",
       " 'googletagservices',\n",
       " 'gpt',\n",
       " 'grayscale',\n",
       " 'gs',\n",
       " 'gscontxt',\n",
       " 'gsurl',\n",
       " 'has',\n",
       " 'hatena',\n",
       " 'haveyoursay',\n",
       " 'headerbutton',\n",
       " 'held',\n",
       " 'hf',\n",
       " 'hits',\n",
       " 'homepage',\n",
       " 'homes',\n",
       " 'honours',\n",
       " 'hostname',\n",
       " 'hours',\n",
       " 'href',\n",
       " 'html',\n",
       " 'http',\n",
       " 'https',\n",
       " 'icons',\n",
       " 'idcta',\n",
       " 'ignored',\n",
       " 'iife',\n",
       " 'img',\n",
       " 'imrworldwide',\n",
       " 'inbox',\n",
       " 'info',\n",
       " 'init',\n",
       " 'inline',\n",
       " 'instagram',\n",
       " 'instanceof',\n",
       " 'int',\n",
       " 'interviewed',\n",
       " 'invoking',\n",
       " 'iplayer',\n",
       " 'istats',\n",
       " 'items',\n",
       " 'jars',\n",
       " 'javascript',\n",
       " 'jira',\n",
       " 'joined',\n",
       " 'jquery',\n",
       " 'js',\n",
       " 'jsonp',\n",
       " 'jssignals',\n",
       " 'keyframes',\n",
       " 'keyline',\n",
       " 'keywords',\n",
       " 'labels',\n",
       " 'lang',\n",
       " 'lazyload',\n",
       " 'lazyloaded',\n",
       " 'lazyloading',\n",
       " 'len',\n",
       " 'lib',\n",
       " 'lies',\n",
       " 'lifted',\n",
       " 'linejoin',\n",
       " 'livejournal',\n",
       " 'localnews',\n",
       " 'locations',\n",
       " 'logo',\n",
       " 'lotame',\n",
       " 'ls',\n",
       " 'ltr',\n",
       " 'lx',\n",
       " 'lxadverts',\n",
       " 'marks',\n",
       " 'matches',\n",
       " 'max',\n",
       " 'maxlength',\n",
       " 'meneame',\n",
       " 'menuitem',\n",
       " 'metadata',\n",
       " 'migrants',\n",
       " 'ministers',\n",
       " 'minutes',\n",
       " 'missed',\n",
       " 'miterlimit',\n",
       " 'mixi',\n",
       " 'modules',\n",
       " 'moimir',\n",
       " 'morebutton',\n",
       " 'moz',\n",
       " 'mozart',\n",
       " 'mozilla',\n",
       " 'ms',\n",
       " 'msie',\n",
       " 'mybbc',\n",
       " 'nations',\n",
       " 'nav',\n",
       " 'navpromo',\n",
       " 'newsdotcom',\n",
       " 'newsreaders',\n",
       " 'northernireland',\n",
       " 'noscript',\n",
       " 'notifications',\n",
       " 'nowrap',\n",
       " 'ns',\n",
       " 'nw',\n",
       " 'objects',\n",
       " 'occurred',\n",
       " 'odnoklassniki',\n",
       " 'ol',\n",
       " 'oldonload',\n",
       " 'onload',\n",
       " 'onscroll',\n",
       " 'optimizely',\n",
       " 'options',\n",
       " 'org',\n",
       " 'osx',\n",
       " 'padded',\n",
       " 'pageshow',\n",
       " 'params',\n",
       " 'parents',\n",
       " 'pathname',\n",
       " 'paths',\n",
       " 'payloads',\n",
       " 'pe',\n",
       " 'persisted',\n",
       " 'personalisation',\n",
       " 'perspectives',\n",
       " 'photos',\n",
       " 'pictures',\n",
       " 'pieces',\n",
       " 'pinterest',\n",
       " 'pioneered',\n",
       " 'pixels',\n",
       " 'placeholder',\n",
       " 'playing',\n",
       " 'plurk',\n",
       " 'png',\n",
       " 'polyfill',\n",
       " 'preferences',\n",
       " 'prefetch',\n",
       " 'progid',\n",
       " 'promis',\n",
       " 'promo',\n",
       " 'proud',\n",
       " 'provides',\n",
       " 'ptrt',\n",
       " 'pw',\n",
       " 'px',\n",
       " 'pyramids',\n",
       " 'qa',\n",
       " 'qq',\n",
       " 'qualities',\n",
       " 'questioned',\n",
       " 'questions',\n",
       " 'reactid',\n",
       " 'reactroot',\n",
       " 'reeldotcom',\n",
       " 'registers',\n",
       " 'renren',\n",
       " 'reported',\n",
       " 'reports',\n",
       " 'requests',\n",
       " 'restaurants',\n",
       " 'restored',\n",
       " 'rgba',\n",
       " 'says',\n",
       " 'searchbox',\n",
       " 'seconds',\n",
       " 'sections',\n",
       " 'sets',\n",
       " 'settings',\n",
       " 'sharedmodules',\n",
       " 'signed',\n",
       " 'sites',\n",
       " 'slots',\n",
       " 'smp',\n",
       " 'soundcloud',\n",
       " 'sounds',\n",
       " 'specified',\n",
       " 'spellcheck',\n",
       " 'sponsored',\n",
       " 'spying',\n",
       " 'sr',\n",
       " 'src',\n",
       " 'srcset',\n",
       " 'ssa',\n",
       " 'ssc',\n",
       " 'stacked',\n",
       " 'stackoverflow',\n",
       " 'stats',\n",
       " 'statusbar',\n",
       " 'stories',\n",
       " 'str',\n",
       " 'styles',\n",
       " 'stylesheet',\n",
       " 'subnav',\n",
       " 'substring',\n",
       " 'subtree',\n",
       " 'suggestions',\n",
       " 'supercomputer',\n",
       " 'svg',\n",
       " 'swfobject',\n",
       " 'tabindex',\n",
       " 'tabs',\n",
       " 'tags',\n",
       " 'talks',\n",
       " 'templates',\n",
       " 'testfeature',\n",
       " 'thousands',\n",
       " 'thresholds',\n",
       " 'tightens',\n",
       " 'timeout',\n",
       " 'timestamp',\n",
       " 'tp',\n",
       " 'tps',\n",
       " 'trafalgar',\n",
       " 'translations',\n",
       " 'traveldotcom',\n",
       " 'treated',\n",
       " 'trusted',\n",
       " 'tv',\n",
       " 'typeof',\n",
       " 'uas',\n",
       " 'ui',\n",
       " 'uid',\n",
       " 'uk',\n",
       " 'ul',\n",
       " 'unescape',\n",
       " 'unobserve',\n",
       " 'updated',\n",
       " 'uppercase',\n",
       " 'uri',\n",
       " 'url',\n",
       " 'usingthebbc',\n",
       " 'utf',\n",
       " 'utils',\n",
       " 'var',\n",
       " 'versions',\n",
       " 'views',\n",
       " 'visited',\n",
       " 'vkontakte',\n",
       " 'waf',\n",
       " 'waits',\n",
       " 'wants',\n",
       " 'webkit',\n",
       " 'webmodule',\n",
       " 'webmodules',\n",
       " 'website',\n",
       " 'weibo',\n",
       " 'women',\n",
       " 'ws',\n",
       " 'wwhp',\n",
       " 'wwscripts',\n",
       " 'www',\n",
       " 'xl',\n",
       " 'xml',\n",
       " 'xs',\n",
       " 'xxl',\n",
       " 'years',\n",
       " 'youtube']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_bbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to write a regular expression to tokenize text in such a way that the\n",
    "word don’t is tokenized into do and n’t? Explain why this regular expression won’t\n",
    "work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', 't']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"n't|\\w+\", \"don't\")  # + is a greedy quantifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bla', 'do', \"n't\", 'blabla']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\\\bdo|n't\\\\b|\\w+\", \"bla don't blabla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to write code to convert text into hAck3r, using regular expressions and\n",
    "substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize\n",
    "the text to lowercase before converting it. Add more substitutions of your own.\n",
    "Now try to map s to two different values: $ for word-initial s, and 5 for wordinternal\n",
    "s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Examine the results of processing the URL http://news.bbc.co.uk/ \n",
    "using the regular expressions suggested above. You will see that there is still a \n",
    "fair amount of non-textual data there, particularly JavaScript commands.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3xam1n3 th3 r35u|t5 of proc3551ng th3 ur| http://n3w55w33t!bbc5w33t!co5w33t!uk/ \\nu51ng th3 r3gu|ar 3xpr3551on5 5ugg35t3d abov35w33t! you w1|| 533 that th3r3 15 5t1|| a \\nfa1r amount of non-t3xtua| data th3r3, part1cu|ar|y java5cr1pt command55w33t!'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_v01 = (\n",
    "    text\n",
    "    .lower()\n",
    "    .replace('e', '3')\n",
    "    .replace('i', '1')\n",
    "    .replace('l', '|')\n",
    "    .replace('s', '5')\n",
    "    .replace('.', '5w33t!')\n",
    "    .replace('ate', '8')\n",
    ")\n",
    "text_v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3xam1n3 th3 r35u|t5 of proc3551ng th3 ur| http://n3w55w33t!bbc5w33t!co5w33t!uk/ \\nu51ng th3 r3gu|ar 3xpr3551on5 $ugg35t3d abov35w33t! you w1|| $33 that th3r3 15 $t1|| a \\nfa1r amount of non-t3xtua| data th3r3, part1cu|ar|y java5cr1pt command55w33t!'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_v02 = (\n",
    "    text.lower()\n",
    "    .replace('e', '3')\n",
    "    .replace('i', '1')\n",
    "    .replace('l', '|')\n",
    "    .replace('.', '5w33t!')\n",
    "    .replace('ate', '8')\n",
    ")\n",
    "text_v02 = re.sub('\\\\bs', '$', text_v02)\n",
    "text_v02 = re.sub('s', '5', text_v02)\n",
    "text_v02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig Latin is a simple transformation of English text. Each word of the text is\n",
    "converted as follows: move any consonant (or consonant cluster) that appears at\n",
    "the start of the word to the end, then append ay, e.g., string → ingstray, idle →\n",
    "idleay (see http://en.wikipedia.org/wiki/Pig_Latin).<div>\n",
    "a. Write a function to convert a word to Pig Latin.<div>\n",
    "b. Write code that converts text, instead of individual words.<div>\n",
    "c. Extend it further to preserve capitalization, to keep qu together (so that\n",
    "quiet becomes ietquay, for example), and to detect when y is used as a consonant\n",
    "(e.g., yellow) versus a vowel (e.g., style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_pig_latin(word):\n",
    "    result = word\n",
    "    if word.isalpha():\n",
    "        pattern = re.compile('^(y|qu|[^aeiouy]*)', re.IGNORECASE)\n",
    "        beginning = re.findall(pattern, word)[0]\n",
    "        stem = word.replace(beginning, '', 1)\n",
    "        result = stem + beginning + 'ay'\n",
    "        # preserve capitalization\n",
    "        result = result.title() if word.istitle() else result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_pig_latin(text):\n",
    "    return ' '.join(list(map(word_to_pig_latin, nltk.word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ingstray', 'idleay', '000...', 'ellowyay', 'ylestay', 'Ellowyay']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(word_to_pig_latin, ['string', 'idle', '000...', 'yellow', 'style', 'Yellow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: \n",
    "move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Igpay Atinlay isay aay implesay ansformationtray ofay Englishay exttay . Eachay ordway ofay ethay exttay isay onvertedcay asay ollowsfay : ovemay anyay onsonantcay ( oray onsonantcay usterclay ) atthay appearsay atay ethay artstay ofay ethay ordway otay ethay enday , enthay appenday ayay'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_pig_latin(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some text from a language that has vowel harmony (e.g., Hungarian),\n",
    "extract the vowel sequences of words, and create a vowel bigram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_vowel_harmony = ConditionalFreqDist(\n",
    "    bigram\n",
    "    for w in udhr.words('Turkish_Turkce-UTF8')\n",
    "    for bigram in nltk.bigrams(re.findall('[aıоueiöü]', w.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   ı   о   u   e   i   ö   ü \n",
      "a 205 247   0  21  65  83   0   6 \n",
      "ı  97  64   0   1   2   1   0   0 \n",
      "о   0   0   0   0   0   0   0   0 \n",
      "u  62   0   0  72   6   3   0   3 \n",
      "e  56   0   0   4 246 236   0   0 \n",
      "i  58   0   0   3 189 156   0   0 \n",
      "ö   0   0   0   0  30   0   2   7 \n",
      "ü   7   0   0   0  34  13   0  26 \n"
     ]
    }
   ],
   "source": [
    "turkish_vowel_harmony.tabulate(conditions='aıоueiöü', samples='aıоueiöü')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turkish vowel harmony was confirmed.<div>\n",
    "'a' -> ('a', 'ı')<div>\n",
    "'ı' -> ('a', 'ı')<div>\n",
    "'u' -> ('a', 'u')<div>\n",
    "'e' -> ('e', 'i')<div>\n",
    "'i' -> ('e', 'i')<div>\n",
    "'ö' -> 'e'<div>\n",
    "'ü' -> ('e', 'ü', 'i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s random module includes a function choice() which randomly chooses\n",
    "an item from a sequence; e.g., choice(\"aehh \") will produce one of four possible\n",
    "characters, with the letter h being twice as frequent as the others. Write a generator\n",
    "expression that produces a sequence of 500 randomly chosen letters drawn from\n",
    "the string \"aehh \", and put this expression inside a call to the ''.join() function,\n",
    "to concatenate them into one long string. You should get a result that looks like\n",
    "uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split()\n",
    "and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h haah ahh hahe ha ah hhhhah eh eh hhh ea hhheha ehhaheahhaaahhhah hhhhhe eaha hahhaeahhhhhh he aeh ahehe hehhehha eahhhahheh ah he a h ehhh ahaea aha eaahe eeaahaa ahhehaehhhah ee hhe ea hh he hee he heeeeaahhh eeh e hahhh hhhheehhh ehhhee hheaaa hhea h ehhhhhhhaehhh eahheh eheahhhea hhehhheeh a hheeeeh h hhhah h hh hheaa hhehhhhehhhh aeeh ha heeh aea hhh eaeh ehh aa h eheh h ahehaah aah ahhea ehe hh h ahaheeaea heahaahhhahhh hhahaaheheeeheeh hh ha h heaahhah ahehhh a'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(''.join(np.random.choice(list(\"aehh \"), 500)).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the numeric expressions in the following sentence from the MedLine\n",
    "Corpus: <b>The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15%\n",
    "and 8.16 +/- 0.23%, respectively.</b> Should we say that the numeric expression 4.53\n",
    "+/- 0.15% is three words? Or should we say that it’s a single compound word? Or\n",
    "should we say that it is actually nine words, since it’s read “four point five three,\n",
    "plus or minus fifteen percent”? Or should we say that it’s not a “real” word at all,\n",
    "since it wouldn’t appear in any dictionary? Discuss these different possibilities. Can\n",
    "you think of application domains that motivate at least two of these answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation depends on our text analysis task. When we want to build a voice assistant we should process this structure as nine words. If we want to parse numbers we should treat <b>4.53 +/- 0.15%</b> as two words. If our goal is to classify text we can even get rid of all digits as well as any punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability measures are used to score the reading difficulty of a text, for the\n",
    "purposes of selecting texts of appropriate difficulty for language learners. Let us\n",
    "define μw to be the average number of letters per word, and μs to be the average\n",
    "number of words per sentence, in a given text. The Automated Readability Index\n",
    "(ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score\n",
    "for various sections of the Brown Corpus, including section f (popular lore) and\n",
    "j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence\n",
    "of words, whereas nltk.corpus.brown.sents() produces a sequence of\n",
    "sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ari(words, sents):\n",
    "    mu_w = sum(len(w) for w in words) / len(words)\n",
    "    mu_s = sum(len(s) for s in sents) / len(sents)\n",
    "    return 4.71 * mu_w + 0.5 * mu_s - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_df = pd.DataFrame(\n",
    "    data=[(category, get_ari(brown.words(categories=category),\n",
    "                             brown.sents(categories=category)))\n",
    "          for category in brown.categories()],\n",
    "    columns=['category', 'ari']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_df.sort_values('ari', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mystery</td>\n",
       "      <td>3.833552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventure</td>\n",
       "      <td>4.084168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>romance</td>\n",
       "      <td>4.349224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fiction</td>\n",
       "      <td>4.910474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>science_fiction</td>\n",
       "      <td>4.978058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>humor</td>\n",
       "      <td>7.887805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hobbies</td>\n",
       "      <td>8.922356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>editorial</td>\n",
       "      <td>9.471025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news</td>\n",
       "      <td>10.176685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>religion</td>\n",
       "      <td>10.203110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lore</td>\n",
       "      <td>10.254756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reviews</td>\n",
       "      <td>10.769700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>belles_lettres</td>\n",
       "      <td>10.987653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>learned</td>\n",
       "      <td>11.926007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>government</td>\n",
       "      <td>12.084303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category        ari\n",
       "9           mystery   3.833552\n",
       "0         adventure   4.084168\n",
       "13          romance   4.349224\n",
       "3           fiction   4.910474\n",
       "14  science_fiction   4.978058\n",
       "6             humor   7.887805\n",
       "5           hobbies   8.922356\n",
       "2         editorial   9.471025\n",
       "10             news  10.176685\n",
       "11         religion  10.203110\n",
       "8              lore  10.254756\n",
       "12          reviews  10.769700\n",
       "1    belles_lettres  10.987653\n",
       "7           learned  11.926007\n",
       "4        government  12.084303"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ari_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\n",
    "on each word. Do the same thing with the Lancaster Stemmer, and see if you observe\n",
    "any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer  = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', 'And', 'his', 'eyes', '--', 'those', 'miniature', 'sundials', 'of', 'variegated', 'yellow', '--', 'had', 'not', 'altered', 'their', 'expression', 'or', 'direction', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = brown.sents(categories='romance')[42]\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', 'and', 'hi', 'eye', '--', 'those', 'miniatur', 'sundial', 'of', 'varieg', 'yellow', '--', 'had', 'not', 'alter', 'their', 'express', 'or', 'direct', '.']\n"
     ]
    }
   ],
   "source": [
    "porter_output = [porter_stemmer.stem(w) for w in sent]\n",
    "print(porter_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', 'and', 'his', 'ey', '--', 'thos', 'miny', 'sund', 'of', 'varieg', 'yellow', '--', 'had', 'not', 'alt', 'their', 'express', 'or', 'direct', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster_output = [lancaster_stemmer.stem(w) for w in sent]\n",
    "print(lancaster_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster stemmer has successfully stemmed 'his' to 'his' while Porter hasn't. Lancaster stemmer tries to stem a word more strongly than Porter stemmer ('sundials' -> 'sund', 'eyes' -> 'ey', 'miniature' -> 'miny', etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variable saying to contain the list ['After', 'all', 'is', 'said',\n",
    "'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list using a for loop, and store the result in a new list lengths. Hint: begin by assigning\n",
    "the empty list to lengths, using lengths = []. Then each time through the loop,\n",
    "use append() to add another length value to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for w in saying:\n",
    "    lengths.append(len(w))\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable silly to contain the string: 'newly formed bland ideas are\n",
    "inexpressible in an infuriating way'. (This happens to be the legitimate interpretation\n",
    "that bilingual English-Spanish speakers can assign to Chomsky’s famous\n",
    "nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now\n",
    "write code to perform the following tasks:<div>\n",
    "a. Split silly into a list of strings, one per word, using Python’s split() operation,\n",
    "and save this to a variable called bland.<div>\n",
    "b. Extract the second letter of each word in silly and join them into a string, to\n",
    "get 'eoldrnnnna'.<div>\n",
    "c. Combine the words in bland back into a single string, using join(). Make sure\n",
    "the words in the resulting string are separated with whitespace.<div>\n",
    "d. Print the words of silly in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "bland = silly.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([w[1] for w in bland])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "are\n",
      "bland\n",
      "formed\n",
      "ideas\n",
      "in\n",
      "inexpressible\n",
      "infuriating\n",
      "newly\n",
      "way\n"
     ]
    }
   ],
   "source": [
    "for w in sorted(bland):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index() function can be used to look up items in sequences. For example,\n",
    "'inexpressible'.index('e') tells us the index of the first position of the letter e. <div>\n",
    "a. What happens when you look up a substring, e.g., 'inexpressible'.index('re')?<div>\n",
    "b. Define a variable words containing a list of words. Now use words.index() to\n",
    "look up the position of an individual word.<div>\n",
    "c. Define a variable silly as in Exercise 32. Use the index() function in combination\n",
    "with list slicing to build a list phrase consisting of all the words up to\n",
    "(but not including) in in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['aaa', 'bbb', 'ccc']\n",
    "words.index('bbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = silly.split()\n",
    "phrase = words[:words.index('in')]\n",
    "phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to convert nationality adjectives such as Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjective2country(w):\n",
    "    words = set(wordnet.words())\n",
    "    if w.endswith('ian'):\n",
    "        stem = w[:-len('ian')]\n",
    "        for suffix in ['ium', 'ia', 'a', 'i', '']:\n",
    "            if (stem + suffix).lower() in words:\n",
    "                return stem + suffix\n",
    "    elif w.endswith('an'):\n",
    "        stem = w[:-len('an')]\n",
    "        for suffix in ['a', '']:\n",
    "            if (stem + suffix).lower() in words:\n",
    "                return stem + suffix\n",
    "    elif w.endswith('ese'):\n",
    "        stem = w[:-len('ese')]\n",
    "        for suffix in ['a', '']:\n",
    "            if (stem + suffix).lower() in words:\n",
    "                return stem + suffix\n",
    "    elif w.endswith('ish'):\n",
    "        stem = w[:-len('ish')]\n",
    "        for suffix in ['eland', 'land', 'mark', 'and', 'en', 'ey']:\n",
    "            if (stem + suffix).lower() in words:\n",
    "                return stem + suffix\n",
    "    # And more...\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Turkey', 'China', 'Canada')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjective2country('Turkish'), adjective2country('Chinese'), adjective2country('Canadian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the LanguageLog post on phrases of the form as best as p can and as best p\n",
    "can, where p is a pronoun. Investigate this phenomenon with the help of a corpus\n",
    "and the findall() method for searching tokenized text described in Section 3.5.\n",
    "The post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = ['I', 'you', 'he', 'she', 'it', 'we', 'they']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As best as I could\n"
     ]
    }
   ],
   "source": [
    "text.findall('<[Aa]s><best><as><{}><can|could>'.format('|'.join(pronouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as best he could; as best I could; as best I could\n"
     ]
    }
   ],
   "source": [
    "text.findall('<[Aa]s><best><{}><can|could>'.format('|'.join(pronouns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the lolcat version of the book of Genesis, accessible as nltk.corpus.gene\n",
    "sis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://\n",
    "www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions\n",
    "to convert English words into corresponding lolspeak words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heavens and the earth.\n",
      "Now the earth was formless and empty.  Darkness was on the surface\n",
      "of the deep.  God's Spirit was hovering over the surface\n",
      "of the waters.\n",
      "God said, \"Let there be light,\" and there was light.\n",
      "God saw the light, and saw that it was good.  God divided\n",
      "the light from the darkness.\n",
      "God called the light Day, and the darkness he called Night.\n",
      "There was evening and there was morning, one day.\n",
      "God said, \"Let there be an expanse in the middle of the\n"
     ]
    }
   ],
   "source": [
    "text = genesis.raw('english-web.txt')[:500]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions = [\n",
    "    ('ight\\\\b', 'iet'),\n",
    "    ('\\\\bI\\\\b', 'ai'),\n",
    "    ('\\\\bhe\\\\b', 'him'),\n",
    "    ('\\\\bhis\\\\b', 'him'),\n",
    "    ('\\\\bshe\\\\b', 'her'),\n",
    "    ('\\\\bhers\\\\b', 'her'),\n",
    "    ('\\\\bthey\\\\b', 'dem'),\n",
    "    ('\\\\btheir\\\\b', 'dem'),\n",
    "    ('\\\\bthem\\\\b', 'dem'),\n",
    "    ('\\\\bthe\\\\b', 'teh'),\n",
    "    ('th', 'f'),\n",
    "    ('\\\\bam\\\\b', 'iz'),\n",
    "    ('\\\\bme\\\\b', 'meh'),\n",
    "    ('\\\\byou\\\\b', 'yu'),\n",
    "    ('ee\\\\b', 'e'),\n",
    "    ('ee', 'ea'),\n",
    "    ('le\\\\b', 'el'),\n",
    "    ('oa', 'ow'),\n",
    "    ('er\\\\b', 'ah'),\n",
    "    ('ing\\\\b', 'in')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern, repl in conversions:\n",
    "    text = re.sub(pattern, repl, text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In teh beginnin God created teh heavens and teh earf.\n",
      "Now teh earf was formless and empty.  Darkness was on teh surface\n",
      "of teh deap.  God's Spirit was hoverin ovah teh surface\n",
      "of teh waters.\n",
      "God said, \"Let fere be liet,\" and fere was liet.\n",
      "God saw teh liet, and saw fat it was good.  God divided\n",
      "teh liet from teh darkness.\n",
      "God called teh liet Day, and teh darkness him called Niet.\n",
      "fere was evenin and fere was mornin, one day.\n",
      "God said, \"Let fere be an expanse in teh middel of teh\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about the re.sub() function for string substitution using regular expressions,\n",
    "using help(re.sub) and by consulting the further readings for this chapter.\n",
    "Use re.sub in writing code to remove HTML tags from an HTML file, and to\n",
    "normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sub in module re:\n",
      "\n",
      "sub(pattern, repl, string, count=0, flags=0)\n",
      "    Return the string obtained by replacing the leftmost\n",
      "    non-overlapping occurrences of the pattern in string by the\n",
      "    replacement repl.  repl can be either a string or a callable;\n",
      "    if a string, backslash escapes in it are processed.  If it is\n",
      "    a callable, it's passed the match object and must return\n",
      "    a replacement string to be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(re.sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('http://www.nltk.org/').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n  \\n    \\n    Natural Language Toolkit &#8212; NLTK 3.4.4 documentation\\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n     \\n  \\n    \\n      \\n        NLTK 3.4.4 documentation\\n        \\n          next |\\n          modules |\\n          index\\n        \\n       \\n    \\n\\n    \\n      \\n        \\n            \\n      \\n        \\n          \\n            \\n  \\nNatural Language Toolkit¶\\nNLTK is a leading platform for building Python programs to work with human language data.\\nIt provides easy-to-use interfaces to over 50 corpora and lexical\\nresources such as WordNet,\\nalong with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\\nwrappers for industrial-strength NLP libraries,\\nand an active discussion forum.\\nThanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\\nNLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\\nNLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\\nNLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\\nand “an amazing library to play with natural language.”\\nNatural Language Processing with Python provides a practical\\nintroduction to programming for language processing.\\nWritten by the creators of NLTK, it guides the reader through the fundamentals\\nof writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\\nand more.\\nThe online version of the book has been been updated for Python 3 and NLTK 3.\\n(The original Python 2 version is still available at http://nltk.org/book_1ed.)\\n\\nSome simple things you can do with NLTK¶\\nTokenize and tag some text:\\n&gt;&gt;&gt; import nltk\\n&gt;&gt;&gt; sentence = &quot;&quot;&quot;At eight o&#39;clock on Thursday morning\\n... Arthur didn&#39;t feel very good.&quot;&quot;&quot;\\n&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)\\n&gt;&gt;&gt; tokens\\n[&#39;At&#39;, &#39;eight&#39;, &quot;o&#39;clock&quot;, &#39;on&#39;, &#39;Thursday&#39;, &#39;morning&#39;,\\n&#39;Arthur&#39;, &#39;did&#39;, &quot;n&#39;t&quot;, &#39;feel&#39;, &#39;very&#39;, &#39;good&#39;, &#39;.&#39;]\\n&gt;&gt;&gt; tagged = nltk.pos_tag(tokens)\\n&gt;&gt;&gt; tagged[0:6]\\n[(&#39;At&#39;, &#39;IN&#39;), (&#39;eight&#39;, &#39;CD&#39;), (&quot;o&#39;clock&quot;, &#39;JJ&#39;), (&#39;on&#39;, &#39;IN&#39;),\\n(&#39;Thursday&#39;, &#39;NNP&#39;), (&#39;morning&#39;, &#39;NN&#39;)]\\n\\n\\nIdentify named entities:\\n&gt;&gt;&gt; entities = nltk.chunk.ne_chunk(tagged)\\n&gt;&gt;&gt; entities\\nTree(&#39;S&#39;, [(&#39;At&#39;, &#39;IN&#39;), (&#39;eight&#39;, &#39;CD&#39;), (&quot;o&#39;clock&quot;, &#39;JJ&#39;),\\n           (&#39;on&#39;, &#39;IN&#39;), (&#39;Thursday&#39;, &#39;NNP&#39;), (&#39;morning&#39;, &#39;NN&#39;),\\n       Tree(&#39;PERSON&#39;, [(&#39;Arthur&#39;, &#39;NNP&#39;)]),\\n           (&#39;did&#39;, &#39;VBD&#39;), (&quot;n&#39;t&quot;, &#39;RB&#39;), (&#39;feel&#39;, &#39;VB&#39;),\\n           (&#39;very&#39;, &#39;RB&#39;), (&#39;good&#39;, &#39;JJ&#39;), (&#39;.&#39;, &#39;.&#39;)])\\n\\n\\nDisplay a parse tree:\\n&gt;&gt;&gt; from nltk.corpus import treebank\\n&gt;&gt;&gt; t = treebank.parsed_sents(&#39;wsj_0001.mrg&#39;)[0]\\n&gt;&gt;&gt; t.draw()\\n\\n\\n\\nNB. If you publish work that uses NLTK, please cite the NLTK book as\\nfollows:\\n\\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\\n\\n\\n\\nNext Steps¶\\n\\nsign up for release announcements\\njoin in the discussion\\n\\n\\n\\n\\nContents¶\\n\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n\\nIndex\\nModule Index\\nSearch Page\\n\\n\\n\\n\\n          \\n        \\n      \\n        \\n        \\n          Table of Contents\\n          \\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n          \\n            Search\\n            \\n                \\n                \\n            \\n          \\n        \\n        \\n      \\n    \\n\\n    \\n      \\n        \\n          \\n            next |\\n            modules |\\n            index\\n          \\n          \\n              \\n              Show Source\\n          \\n        \\n\\n        \\n          \\n    \\n        &#169; Copyright 2019, NLTK Project.\\n      Last updated on Jul 04, 2019.\\n      Created using Sphinx 2.1.2.\\n    \\n        \\n        \\n      \\n    \\n\\n  \\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_cleaned = re.sub('<.*?>', '', html, flags=re.DOTALL)\n",
    "html_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Toolkit &#8212; NLTK 3.4.4 documentation NLTK 3.4.4 documentation next | modules | index Natural Language Toolkit¶ NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project. NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.” Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more. The online version of the book has been been updated for Python 3 and NLTK 3. (The original Python 2 version is still available at http://nltk.org/book_1ed.) Some simple things you can do with NLTK¶ Tokenize and tag some text: &gt;&gt;&gt; import nltk &gt;&gt;&gt; sentence = &quot;&quot;&quot;At eight o&#39;clock on Thursday morning ... Arthur didn&#39;t feel very good.&quot;&quot;&quot; &gt;&gt;&gt; tokens = nltk.word_tokenize(sentence) &gt;&gt;&gt; tokens [&#39;At&#39;, &#39;eight&#39;, &quot;o&#39;clock&quot;, &#39;on&#39;, &#39;Thursday&#39;, &#39;morning&#39;, &#39;Arthur&#39;, &#39;did&#39;, &quot;n&#39;t&quot;, &#39;feel&#39;, &#39;very&#39;, &#39;good&#39;, &#39;.&#39;] &gt;&gt;&gt; tagged = nltk.pos_tag(tokens) &gt;&gt;&gt; tagged[0:6] [(&#39;At&#39;, &#39;IN&#39;), (&#39;eight&#39;, &#39;CD&#39;), (&quot;o&#39;clock&quot;, &#39;JJ&#39;), (&#39;on&#39;, &#39;IN&#39;), (&#39;Thursday&#39;, &#39;NNP&#39;), (&#39;morning&#39;, &#39;NN&#39;)] Identify named entities: &gt;&gt;&gt; entities = nltk.chunk.ne_chunk(tagged) &gt;&gt;&gt; entities Tree(&#39;S&#39;, [(&#39;At&#39;, &#39;IN&#39;), (&#39;eight&#39;, &#39;CD&#39;), (&quot;o&#39;clock&quot;, &#39;JJ&#39;), (&#39;on&#39;, &#39;IN&#39;), (&#39;Thursday&#39;, &#39;NNP&#39;), (&#39;morning&#39;, &#39;NN&#39;), Tree(&#39;PERSON&#39;, [(&#39;Arthur&#39;, &#39;NNP&#39;)]), (&#39;did&#39;, &#39;VBD&#39;), (&quot;n&#39;t&quot;, &#39;RB&#39;), (&#39;feel&#39;, &#39;VB&#39;), (&#39;very&#39;, &#39;RB&#39;), (&#39;good&#39;, &#39;JJ&#39;), (&#39;.&#39;, &#39;.&#39;)]) Display a parse tree: &gt;&gt;&gt; from nltk.corpus import treebank &gt;&gt;&gt; t = treebank.parsed_sents(&#39;wsj_0001.mrg&#39;)[0] &gt;&gt;&gt; t.draw() NB. If you publish work that uses NLTK, please cite the NLTK book as follows: Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc. Next Steps¶ sign up for release announcements join in the discussion Contents¶ NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Index Module Index Search Page Table of Contents NLTK News Installing NLTK Installing NLTK Data Contribute to NLTK FAQ Wiki API HOWTO Search next | modules | index Show Source &#169; Copyright 2019, NLTK Project. Last updated on Jul 04, 2019. Created using Sphinx 2.1.2.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_normalized = re.sub('\\s+', ' ', html_cleaned).strip()\n",
    "html_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting challenge for tokenization is words that have been split across a\n",
    "linebreak. E.g., if long-term is split, then we have the string long-\\nterm.<div>\n",
    "a. Write a regular expression that identifies words that are hyphenated at a linebreak.\n",
    "The expression will need to include the \\n character.<div>\n",
    "b. Use re.sub() to remove the \\n character from these words.<div>\n",
    "c. How might you identify words that should not remain hyphenated once the\n",
    "newline is removed, e.g., 'encyclo-\\npedia'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_hyphenated_words(text):\n",
    "    valid_words = set(wordnet.words())\n",
    "    result = text[:]\n",
    "    for w in set(re.findall('\\w+-\\n\\w+', result)):\n",
    "        w_new = w.replace('\\n', '')\n",
    "        if w_new not in valid_words:\n",
    "            w_new = w_new.replace('-', '')\n",
    "        result = result.replace(w, w_new)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"An interesting challenge for tokenization is words that have been split across a linebreak. E.g., if long-term is split, then we have the string long-\\nterm.\n",
    "a. Write a regular expression that identifies words that are hyphenated at a linebreak. The expression will need to include the \\n character.\n",
    "b. Use re.sub() to remove the \\n character from these words.\n",
    "c. How might you identify words that should not remain hyphenated once the newline is removed, e.g., 'encyclo-\\npedia'?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An interesting challenge for tokenization is words that have been split across a linebreak. E.g., if long-term is split, then we have the string long-term.\\na. Write a regular expression that identifies words that are hyphenated at a linebreak. The expression will need to include the \\n character.\\nb. Use re.sub() to remove the \\n character from these words.\\nc. How might you identify words that should not remain hyphenated once the newline is removed, e.g., 'encyclopedia'?\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_hyphenated_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Wikipedia entry on Soundex. Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex_v01(word):\n",
    "    step0 = word[0]\n",
    "    step1 = re.sub('[hw]', '', word[1:])\n",
    "    step20 = re.sub('[bfpv]+', '1', step1)\n",
    "    step21 = re.sub('[cgjkqsxz]+', '2', step20)\n",
    "    step22 = re.sub('[dt]+', '3', step21)\n",
    "    step23 = re.sub('l+', '4', step22)\n",
    "    step24 = re.sub('[mn]+', '5', step23)\n",
    "    step25 = re.sub('r+', '6', step24)\n",
    "    step3 = re.sub('[aeiouy]+', '', step25)\n",
    "    step4 = step0.upper() + step3\n",
    "    step5 = step4[:4].ljust(4, '0')\n",
    "    return step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soundex_v02(word):\n",
    "    step0, step1 = word[0], word[1:]\n",
    "    step2 = step1.translate(\n",
    "        ''.maketrans(\n",
    "            'bfpvcgjkqsxzdtlmnr', \n",
    "            '111122222222334556', \n",
    "            'hw'\n",
    "        )\n",
    "    )\n",
    "    step3 = ''.join(k for k, g in groupby(step2))\n",
    "    step4 = step3.translate(\n",
    "        ''.maketrans('', '', 'aeiouy')\n",
    "    )\n",
    "    step5 = step0.upper() + step4\n",
    "    step6 = step5[:4].ljust(4, '0')\n",
    "    return step6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    'аmmonium', \n",
    "    'implementation', \n",
    "    'Robert', \n",
    "    'Rupert', \n",
    "    'Rubin', \n",
    "    'Ashcraft', \n",
    "    'Ashcroft', \n",
    "    'Tymczak'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['А555', 'I514', 'R163', 'R163', 'R150', 'A261', 'A261', 'T522']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(soundex_v01, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['А555', 'I514', 'R163', 'R163', 'R150', 'A261', 'A261', 'T522']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(soundex_v02, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain raw texts from two or more genres and compute their respective reading\n",
    "difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC\n",
    "Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence\n",
    "segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_df = pd.DataFrame(\n",
    "    data=[\n",
    "        (fileid, get_ari(nltk.word_tokenize(abc.raw(fileid)),\n",
    "                         [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(abc.raw(fileid))]))\n",
    "        for fileid in abc.fileids()\n",
    "    ],\n",
    "    columns=['fileid', 'ari']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rural.txt</td>\n",
       "      <td>12.616620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>science.txt</td>\n",
       "      <td>12.773322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fileid        ari\n",
       "0    rural.txt  12.616620\n",
       "1  science.txt  12.773322"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ari_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the following nested loop as a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "    vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(''.join(char for char in word if char in 'aeiou') for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(re.sub('[^aeiou]', '', word) for word in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use WordNet to create a semantic index for a text collection. Extend the concordance\n",
    "search program in Example 3-1, indexing each word using the offset of\n",
    "its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some\n",
    "of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedTextExercise42(object):\n",
    "    def __init__(self, text, strategy):\n",
    "        self._text = text\n",
    "        self._strategy = strategy\n",
    "        assert self._strategy in ['first_synset', 'first_hypernym'], \\\n",
    "        \"Valid strategy values are ['first_synset', 'first_hypernym']\"\n",
    "        self._offset = (\n",
    "            self._offset_first_synset \n",
    "            if self._strategy == 'first_synset' \n",
    "            else self._offset_first_hypernym\n",
    "        )\n",
    "        self._index = nltk.Index(\n",
    "            (self._offset(word), i)\n",
    "            for (i, word) in enumerate(text)\n",
    "        )\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._offset(word)\n",
    "        wc = width // 4  # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '%*s' % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _offset_first_synset(self, word):\n",
    "        offset = -1\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            offset = synsets[0].offset()\n",
    "        return offset\n",
    "    \n",
    "    def _offset_first_hypernym(self, word):\n",
    "        offset = -1\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            hypernyms = synsets[0].hypernyms()\n",
    "            if hypernyms:\n",
    "                offset = hypernyms[0].offset()\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "grail = nltk.corpus.webtext.words('grail.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = IndexedTextExercise42(grail, strategy='first_synset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thur , son of Uther Pendragon , from the castle of Camelot . King of the Britons \n",
      ": Man . Sorry . What knight live in that castle over there ? DENNIS : I ' m thirt\n",
      "  Arthur , King of the Britons . Who ' s castle is that ? WOMAN : King of the who\n",
      "     . I am in haste . Who lives in that castle ? WOMAN : No one live there . ART\n",
      " my Knights of the Round Table . Who ' s castle is this ? FRENCH GUARD : This is \n",
      "tle is this ? FRENCH GUARD : This is the castle of my master Guy de Loimbard . AR\n",
      "t show us the Grail , we shall take your castle by force ! FRENCH GUARD : You don\n",
      "TOR : Action ! HISTORIAN : Defeat at the castle seems to have utterly disheartene\n",
      "lcome gentle Sir Knight . Welcome to the Castle Anthrax . GALAHAD : The Castle An\n",
      "me to the Castle Anthrax . GALAHAD : The Castle Anthrax ? ZOOT : Yes . Oh , it   \n",
      "        and - a - half , cut off in this castle with no one to protect us . Oooh \n",
      "   Grail ! I have seen it , here in this castle ! DINGO : Oh no . Oh , no        \n",
      "d she must pay the penalty . And here in Castle Anthrax , we have but one punishm\n",
      ". Other kings said I was daft to build a castle on a swamp , but I built it all  \n",
      "you ' re gonna get , lad : the strongest castle in these islands . HERBERT : But \n",
      "    me . I am in the Tall Tower of Swamp Castle .' At last ! A call ! A cry      \n",
      "      I , sir ? Yeah SCENE 16 : [ inside castle ] PRINCESS LUCKY and GIRLS : [ gi\n",
      "RLS : [ giggle giggle giggle ] [ outside castle ] GUEST : ' Morning ! SENTRY # 1 \n",
      "f King Arthur , sir . FATHER : Very nice castle , Camelot . Uh , very good pig co\n",
      "of spirit may find the Holy Grail in the Castle of uuggggggh '. ARTHUR : What ? M\n",
      "gh '. ARTHUR : What ? MAYNARD : '... the Castle of uuggggggh '. BEDEVERE : What i\n",
      " stops ] [ ethereal music ] ARTHUR : The Castle Aaagh . Our quest is at an end ! \n",
      "melot , to open the doors of this sacred castle , to which God Himself has guided\n",
      "Lord , we demand entrance to this sacred castle ! FRENCH GUARD : No chance , Engl\n",
      " not open this door , we shall take this castle by force ! [ splat ] In the name \n"
     ]
    }
   ],
   "source": [
    "text.concordance('palace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = IndexedTextExercise42(grail, strategy='first_hypernym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with you , good Sir Knight , but I must cross this bridge . BLACK KNIGHT : Then \n",
      " good Sir Knight , but I must cross this bridge . BLACK KNIGHT : Then you shall d\n",
      "uiet ! Quiet ! Quiet ! Quiet ! There are ways of telling whether she is a witch .\n",
      "   made of wood ? VILLAGER # 1 : Build a bridge out of her . BEDEVERE : Ah , but \n",
      "    : Who are you who are so wise in the ways of science ? ARTHUR : I am Arthur ,\n",
      "cred quest . If he will give us food and shelter for the night he can join us in \n",
      " each of the knights went their separate ways . Sir Robin rode north , through th\n",
      " not at all afraid to be killed in nasty ways . Brave , brave , brave , brave Sir\n",
      "  is the Grail ?! OLD MAN : Seek you the Bridge of Death . ARTHUR : The Bridge of\n",
      "k you the Bridge of Death . ARTHUR : The Bridge of Death , which leads to the Gra\n",
      "   come and rescue me . I am in the Tall Tower of Swamp Castle .' At last ! A cal\n",
      "tter . FATHER : You fell out of the Tall Tower , you creep ! HERBERT : No , I    \n",
      "    : Let us taunt it ! It may become so cross that it will make a mistake . ARTH\n",
      "  ] GALAHAD : There it is ! ARTHUR : The Bridge of Death ! ROBIN : Oh , great .  \n",
      " here ? ARTHUR : He is the keeper of the Bridge of Death . He asks each traveller\n",
      "questions . ARTHUR : Three questions may cross in safety . ROBIN : What if you ge\n",
      "th you . BRIDGEKEEPER : Stop ! Who would cross the Bridge of Death must answer me\n",
      "RIDGEKEEPER : Stop ! Who would cross the Bridge of Death must answer me these que\n",
      "RIDGEKEEPER : Stop ! Who approacheth the Bridge of Death must answer me these que\n"
     ]
    }
   ],
   "source": [
    "text.concordance('bridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of a multilingual corpus such as the Universal Declaration of\n",
    "Human Rights Corpus (nltk.corpus.udhr), along with NLTK’s frequency distribution\n",
    "and rank correlation functionality (nltk.FreqDist, nltk.spearman_correla\n",
    "tion), develop a system that guesses the language of a previously unseen text. For\n",
    "simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.metrics import spearman_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks():\n",
    "    cfd = ConditionalFreqDist(\n",
    "        (fileid.replace('-Latin1', ''), char.lower())\n",
    "        for fileid in udhr.fileids()\n",
    "        for char in udhr.raw(fileid)\n",
    "        if fileid.endswith('Latin1')\n",
    "        and char.isalpha()\n",
    "    )\n",
    "    ranks = {\n",
    "        lang: [(c, i) for i, (c, f) in enumerate(cfd[lang].most_common())]\n",
    "        for lang in cfd.conditions()\n",
    "    }\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_language(text, ranks):\n",
    "    fd = FreqDist(char.lower() for char in text if char.isalpha())\n",
    "    rank_test = [(c, i) for i, (c, f) in enumerate(fd.most_common())]\n",
    "    return max(\n",
    "        (spearman_correlation(rank_test, ranks[lang]), lang) \n",
    "        for lang in ranks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = get_ranks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_english = \"\"\"\n",
    "With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr),\n",
    "along with NLTK’s frequency distribution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correla tion), \n",
    "develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character \n",
    "encoding and just a few languages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9299999999999999, 'English')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "define_language(text_english, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_german = \"\"\"\n",
    "Julian und Stefan wollen gemeinsam verreisen. Als Urlaubsort haben sie sich Teneriffa ausgesucht. Zusammen kümmern \n",
    "sie sich um die Organisation der Reise. Julian schaut im Internet nach besonders günstigen Flügen. Der Flug ab Hamburg\n",
    "hat den besten Preis. Allerdings müssen sie zwei Stunden Zeit für die Fahrt zum Flughafen einplanen. Stefan kümmert sich \n",
    "um die Unterkunft. Auf Teneriffa gibt es sehr viele Hotels mit unterschiedlichen Preisen. Vieles ist zu teuer, deswegen \n",
    "schaut Stefan erst einmal nach einem Hostel. Allerdings muss man sich dort selbst um die Verpflegung kümmern. Stefan \n",
    "entscheidet sich schließlich für ein günstiges Hotel, das drei Kilometer vom Strand entfernt liegt. Dafür hat das Hotel\n",
    "auf der Internetseite gute Bewertungen. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9265384615384615, 'German_Deutsch')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "define_language(text_german, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_french = \"\"\"\n",
    "La région des Alpes est située à l’Est de la France. Les Alpes sont la chaîne de montagnes la plus haute d’Europe, \n",
    "c’est aussi une frontière naturelle avec d’autres pays européens : la Suisse et l’Italie. La montagne la plus haute \n",
    "des Alpes s’appelle le Mont Blanc, on peut y monter grâce à un téléphérique. Les français aiment beaucoup cette \n",
    "région car ils peuvent y passer leurs vacances en été et en hiver. En effet, en hiver il y a beaucoup de neige dans \n",
    "les Alpes et les touristes peuvent pratiquer le ski, la luge ou le snowboard. Il fait très froid et pour se réchauffer \n",
    "ils mangent des plats typiques et boivent du vin chaud. En été, les touristes aiment se balader dans les montagnes, \n",
    "ils font de la randonnée. Ils peuvent se baigner dans les lacs et observer les animaux sauvages qui vivent dans cette \n",
    "région : les marmottes et les chamois. Mais on peut voir aussi beaucoup d’animaux domestiques pendant l’été : des vaches, \n",
    "des chèvres et des montons. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9442735042735043, 'French_Francais')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "define_language(text_french, ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program that processes a text and discovers cases where a word has been\n",
    "used with a novel sense. For each word, compute the WordNet similarity between\n",
    "all synsets of the word and all synsets of the words in its context. (Note that this\n",
    "is a crude approach; doing it well is a difficult, open research problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(word1, word2):\n",
    "    return max(\n",
    "        [synset1.path_similarity(synset2)\n",
    "         for synset1 in wn.synsets(word1)\n",
    "         for synset2 in wn.synsets(word2)], \n",
    "        default=0, \n",
    "        key=lambda sim: sim if sim else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.sents(categories='romance')\n",
    "scores = defaultdict(lambda : defaultdict(list))\n",
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sentence 1 the word \"noon\" may be used in a novel sense. Its average similarity score is 0.0747.\n",
      "In sentence 1 the word \"boredom\" may be used in a novel sense. Its average similarity score is 0.0824.\n",
      "In sentence 4 the word \"aluminum\" may be used in a novel sense. Its average similarity score is 0.0948.\n",
      "In sentence 4 the word \"trays\" may be used in a novel sense. Its average similarity score is 0.0833.\n",
      "In sentence 4 the word \"cheesecloth\" may be used in a novel sense. Its average similarity score is 0.0857.\n",
      "In sentence 4 the word \"tomato\" may be used in a novel sense. Its average similarity score is 0.0887.\n",
      "In sentence 19 the word \"suitcase\" may be used in a novel sense. Its average similarity score is 0.0811.\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(sents[:20]):\n",
    "    sent[0] = sent[0].lower()\n",
    "    sent = {\n",
    "        word\n",
    "        for word in sent\n",
    "        if word not in stops\n",
    "        and word.isalpha()\n",
    "        and word.islower()\n",
    "    }\n",
    "\n",
    "    if len(sent) < 3:\n",
    "        continue\n",
    "\n",
    "    for word1, word2 in combinations(sent, 2):\n",
    "        top_score = get_similarity(word1, word2)\n",
    "        if top_score:\n",
    "            scores[i][word1].append(top_score)\n",
    "            scores[i][word2].append(top_score)\n",
    "\n",
    "    for word, word_scores in scores[i].items():\n",
    "        avg_score = sum(word_scores) / len(word_scores)\n",
    "        if avg_score < 0.1:\n",
    "            print('In sentence {} the word \"{}\" may be used in '\n",
    "                  'a novel sense. Its average similarity score is {}.'.format(i, word, round(avg_score, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the article on normalization of non-standard words (Sproat et al., 2001),\n",
    "and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit https://github.com/EFord36/normalise and install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drf92\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:253: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from normalise import normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"On\", \"the\", \"13\", \"Feb.\", \"2007\", \",\", \"Theresa\", \"May\", \"MP\", \"announced\",\n",
    "        \"on\", \"ITV\", \"News\", \"that\", \"the\", \"rate\", \"of\", \"childhod\", \"obesity\", \"had\", \"risen\",\n",
    "        \"from\", \"7.3-9.6%\", \"in\", \"just\", \"3\", \"years\", \",\", \"costing\", \"the\", \"Gov.\", \"£20m\", \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATING NSW DICTIONARY\n",
      "-----------------------\n",
      "\n",
      "10 NSWs found\n",
      "\n",
      "TAGGING NSWs\n",
      "------------\n",
      "\n",
      "10 of 10 tagged\n",
      "\n",
      "SPLITTING NSWs\n",
      "--------------\n",
      "\n",
      "0 of 0 split\n",
      "\n",
      "RETAGGING SPLIT NSWs\n",
      "--------------------\n",
      "\n",
      "0 of 0 retagged\n",
      "\n",
      "CLASSIFYING ALPHABETIC NSWs\n",
      "---------------------------\n",
      "\n",
      "5 of 5 classified\n",
      "\n",
      "CLASSIFYING NUMERIC NSWs\n",
      "------------------------\n",
      "\n",
      "5 of 5 classified\n",
      "\n",
      "CLASSIFYING MISCELLANEOUS NSWs\n",
      "------------------------------\n",
      "\n",
      "0 of 0 classified\n",
      "\n",
      "EXPANDING ALPHABETIC NSWs\n",
      "-------------------------\n",
      "\n",
      "5 of 5 expanded\n",
      "\n",
      "EXPANDING NUMERIC NSWs\n",
      "----------------------\n",
      "\n",
      "5 of 5 expanded\n",
      "\n",
      "EXPANDING MISCELLANEOUS NSWs\n",
      "----------------------------\n",
      "\n",
      "0 of 0 expanded\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'the',\n",
       " 'thirteenth of',\n",
       " 'February',\n",
       " 'two thousand and seven',\n",
       " ',',\n",
       " 'Theresa',\n",
       " 'May',\n",
       " 'M P',\n",
       " 'announced',\n",
       " 'on',\n",
       " 'I T V',\n",
       " 'News',\n",
       " 'that',\n",
       " 'the',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'childhood',\n",
       " 'obesity',\n",
       " 'had',\n",
       " 'risen',\n",
       " 'from',\n",
       " 'seven point three to nine point six percent',\n",
       " 'in',\n",
       " 'just',\n",
       " 'three',\n",
       " 'years',\n",
       " ',',\n",
       " 'costing',\n",
       " 'the',\n",
       " 'government',\n",
       " 'twenty million pounds',\n",
       " '.']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalise(text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
