{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What constraints are required to correctly parse word sequences like I am happy and she is happy but not *you is happy or *they am happy? Implement two solutions for the present tense paradigm of the verb be in English, first taking Grammar (6) as your starting point, and then taking Grammar (18) as the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> P_SG1 VP_SG1\n",
    "    S -> P_SG3 VP_SG3\n",
    "    S -> P_PL VP_PL\n",
    "    \n",
    "    VP_SG1 -> V_SG1 ADJ\n",
    "    VP_SG3 -> V_SG3 ADJ\n",
    "    VP_PL -> V_PL ADJ\n",
    "    \n",
    "    P_SG1 -> 'I'\n",
    "    P_SG3 -> 'he' | 'she' | 'it'\n",
    "    P_PL -> 'they' | 'we' | 'you'\n",
    "    V_SG1 -> 'am'\n",
    "    V_SG3 -> 'is'\n",
    "    V_PL -> 'are'\n",
    "    ADJ -> 'happy'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"I am happy\".split()\n",
    "sent2 = \"she is happy\".split()\n",
    "sent3 = \"you is happy\".split()\n",
    "sent4 = \"they am happy\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser1 = nltk.ChartParser(grammar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (P_SG1 I) (VP_SG1 (V_SG1 am) (ADJ happy)))\n",
      "(S (P_SG3 she) (VP_SG3 (V_SG3 is) (ADJ happy)))\n"
     ]
    }
   ],
   "source": [
    "for sent in [sent1, sent2, sent3, sent4]:\n",
    "    for tree in parser1.parse(sent):\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar2 = nltk.grammar.FeatureGrammar.fromstring(\"\"\"   \n",
    "    S -> P[AGR=?a] VP[AGR=?a]\n",
    "    \n",
    "    VP[AGR=?a] -> V[AGR=?a] ADJ \n",
    "    \n",
    "    P[AGR=[NUM=sg, PER=1]] -> 'I'\n",
    "    P[AGR=[NUM=sg, PER=3]] -> 'he' | 'she' | 'it'\n",
    "    P[AGR=[NUM=pl]] -> 'they' | 'we' | 'you'\n",
    "    V[AGR=[NUM=sg, PER=1]] -> 'am'\n",
    "    V[AGR=[NUM=sg, PER=3]] -> 'is'\n",
    "    V[AGR=[NUM=pl]] -> 'are'\n",
    "    ADJ -> 'happy'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser2 = nltk.FeatureEarleyChartParser(grammar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (P[AGR=[NUM='sg', PER=1]] I)\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (V[AGR=[NUM='sg', PER=1]] am)\n",
      "    (ADJ[] happy)))\n",
      "(S[]\n",
      "  (P[AGR=[NUM='sg', PER=3]] she)\n",
      "  (VP[AGR=[NUM='sg', PER=3]]\n",
      "    (V[AGR=[NUM='sg', PER=3]] is)\n",
      "    (ADJ[] happy)))\n"
     ]
    }
   ],
   "source": [
    "for sent in [sent1, sent2, sent3, sent4]:\n",
    "    for tree in parser2.parse(sent):\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a variant of grammar in 1.1 that uses a feature count to make the distinctions shown below:\n",
    "\n",
    "(54)\t\n",
    "a. The boy sings.\t\n",
    "b. *Boy sings.\n",
    "\n",
    "(55)\t\n",
    "a. The boys sing.\t\n",
    "b. Boys sing.\n",
    "\n",
    "(56)\t\n",
    "a. The boys sing.\t\n",
    "b. Boys sing.\n",
    "\n",
    "(57)\t\n",
    "a. The water is precious.\t\n",
    "b.Water is precious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.FeatureGrammar.fromstring(\"\"\"   \n",
    "    S -> NP[NUM=?n] VP[NUM=?n]\n",
    "    \n",
    "    VP[NUM=?n] -> V[NUM=?n] | V[NUM=?n] ADJ\n",
    "    NP[NUM=?n] -> N[NUM=?n] | DET N[NUM=?n]\n",
    "    \n",
    "    V[NUM=sg] -> 'sings' | 'is'\n",
    "    V[NUM=pl] -> 'sing'\n",
    "    N[NUM=sg] -> 'boy' | 'Boy' | 'water' | 'Water'\n",
    "    N[NUM=pl] -> 'boys' | 'Boys'\n",
    "    DET -> 'The'\n",
    "    ADJ -> 'precious'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.FeatureEarleyChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The boy sings\".split()\n",
    "sent2 = \"Boy sings\".split()\n",
    "sent3 = \"The boys sing\".split()\n",
    "sent4 = \"Boys sing\".split()\n",
    "sent5 = \"The water is precious\".split()\n",
    "sent6 = \"Water is precious\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[NUM='sg'] (DET[] The) (N[NUM='sg'] boy))\n",
      "  (VP[NUM='sg'] (V[NUM='sg'] sings)))\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (N[NUM='sg'] Boy))\n",
      "  (VP[NUM='sg'] (V[NUM='sg'] sings)))\n",
      "(S[]\n",
      "  (NP[NUM='pl'] (DET[] The) (N[NUM='pl'] boys))\n",
      "  (VP[NUM='pl'] (V[NUM='pl'] sing)))\n",
      "(S[]\n",
      "  (NP[NUM='pl'] (N[NUM='pl'] Boys))\n",
      "  (VP[NUM='pl'] (V[NUM='pl'] sing)))\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (DET[] The) (N[NUM='sg'] water))\n",
      "  (VP[NUM='sg'] (V[NUM='sg'] is) (ADJ[] precious)))\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (N[NUM='sg'] Water))\n",
      "  (VP[NUM='sg'] (V[NUM='sg'] is) (ADJ[] precious)))\n"
     ]
    }
   ],
   "source": [
    "for sent in [sent1, sent2, sent3, sent4, sent5, sent6]:\n",
    "    for tree in parser.parse(sent):\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function subsumes() which holds of two feature structures fs1 and fs2 just in case fs1 subsumes fs2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')\n",
    "fs2 = nltk.FeatStruct(CITY='Paris')\n",
    "fs3 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal', CITY='Paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsumes(fs1, fs2):\n",
    "    return fs1.unify(fs2) == fs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsumes(fs1, fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsumes(fs1, fs3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the grammar illustrated in (28) to incorporate a bar feature for dealing with phrasal projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.FeatureGrammar.fromstring(\"\"\"\n",
    "    VP[TENSE=?t, NUM=?n, BAR=1] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n, BAR=0]\n",
    "    VP[TENSE=?t, NUM=?n, BAR=1] -> V[SUBCAT=trans, TENSE=?t, NUM=?n, BAR=0] NP\n",
    "    VP[TENSE=?t, NUM=?n, BAR=1] -> V[SUBCAT=clause, TENSE=?t, NUM=?n, BAR=0] SBar\n",
    "\n",
    "    V[SUBCAT=intrans, TENSE=pres, NUM=sg, BAR=0] -> 'disappears' | 'walks'\n",
    "    V[SUBCAT=trans, TENSE=pres, NUM=sg, BAR=0] -> 'sees' | 'likes'\n",
    "    V[SUBCAT=clause, TENSE=pres, NUM=sg, BAR=0] -> 'says' | 'claims'\n",
    "\n",
    "    V[SUBCAT=intrans, TENSE=pres, NUM=pl, BAR=0] -> 'disappear' | 'walk'\n",
    "    V[SUBCAT=trans, TENSE=pres, NUM=pl, BAR=0] -> 'see' | 'like'\n",
    "    V[SUBCAT=clause, TENSE=pres, NUM=pl, BAR=0] -> 'say' | 'claim'\n",
    "\n",
    "    V[SUBCAT=intrans, TENSE=past, NUM=?n, BAR=0] -> 'disappeared' | 'walked'\n",
    "    V[SUBCAT=trans, TENSE=past, NUM=?n, BAR=0] -> 'saw' | 'liked'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the German grammar in 3.2 to incorporate the treatment of subcategorization presented in 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.FeatureGrammar.fromstring(\"\"\"\n",
    "    # Grammar Productions\n",
    "    S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
    "    NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
    "    NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
    "    VP[AGR=?a] -> V[SUBCAT=intrans, AGR=?a]\n",
    "    VP[AGR=?a] -> V[SUBCAT=trans, OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
    "    # Lexical Productions\n",
    "    # Singular determiners\n",
    "    # masc\n",
    "    Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n",
    "    Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
    "    Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
    "    # fem\n",
    "    Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    "    Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
    "    Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    "    # Plural determiners\n",
    "    Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    "    Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n",
    "    Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    "    # Nouns\n",
    "    N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
    "    N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "    N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
    "    N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "    N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
    "    N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
    "    # Pronouns\n",
    "    PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
    "    PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
    "    PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
    "    PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
    "    PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
    "    PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
    "    PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "    PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "    PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
    "    PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
    "    # Verbs\n",
    "    V[SUBCAT=intrans, AGR=[NUM=sg,PER=1]] -> 'komme'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (V[AGR=[NUM='sg', PER=1], OBJCASE='dat', SUBCAT='trans'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n"
     ]
    }
   ],
   "source": [
    "sent = 'ich folge den Katzen'.split()\n",
    "parser = nltk.FeatureEarleyChartParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a feature based grammar that will correctly describe the following Spanish noun phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.FeatureGrammar.fromstring(\"\"\"\n",
    "    NP -> DET[NUM=?n, GND=?g] N[NUM=?n] ADJ[NUM=?n, GND=?g]\n",
    "    \n",
    "    N[NUM=sg] -> 'cuadro' | 'cortina'\n",
    "    N[NUM=pl] -> 'cuadros' | 'cortinas'\n",
    "    \n",
    "    ADJ[NUM=sg, GND=masc] -> 'hermoso'\n",
    "    ADJ[NUM=pl, GND=masc] -> 'hermosos'\n",
    "    ADJ[NUM=sg, GND=fem] -> 'hermosa'\n",
    "    ADJ[NUM=pl, GND=fem] -> 'hermosas'\n",
    "    \n",
    "    DET[NUM=sg, GND=masc] -> 'un'\n",
    "    DET[NUM=pl, GND=masc] ->  'unos'\n",
    "    DET[NUM=sg, GND=fem] -> 'una'\n",
    "    DET[NUM=pl, GND=fem] -> 'unas'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'un cuadro hermoso'.split()\n",
    "sent2 = 'unos cuadros hermosos'.split()\n",
    "sent3 = 'una cortina hermosa'.split()\n",
    "sent4 = 'unas cortinas hermosas'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.FeatureEarleyChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP[]\n",
      "  (DET[GND='masc', NUM='sg'] un)\n",
      "  (N[NUM='sg'] cuadro)\n",
      "  (ADJ[GND='masc', NUM='sg'] hermoso))\n",
      "(NP[]\n",
      "  (DET[GND='masc', NUM='pl'] unos)\n",
      "  (N[NUM='pl'] cuadros)\n",
      "  (ADJ[GND='masc', NUM='pl'] hermosos))\n",
      "(NP[]\n",
      "  (DET[GND='fem', NUM='sg'] una)\n",
      "  (N[NUM='sg'] cortina)\n",
      "  (ADJ[GND='fem', NUM='sg'] hermosa))\n",
      "(NP[]\n",
      "  (DET[GND='fem', NUM='pl'] unas)\n",
      "  (N[NUM='pl'] cortinas)\n",
      "  (ADJ[GND='fem', NUM='pl'] hermosas))\n"
     ]
    }
   ],
   "source": [
    "for sent in [sent1, sent2, sent3, sent4]:\n",
    "    for tree in parser.parse(sent):\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop your own version of the EarleyChartParser which only prints a trace if the input sequence fails to parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the feature structures shown in 6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
    "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
    "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
    "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
    "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work out on paper what the result is of the following unifications. (Hint: you might find it useful to draw the graph structures.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. fs1 and fs2\n",
    "2. fs1 and fs3\n",
    "3. fs4 and fs5\n",
    "4. fs5 and fs6\n",
    "5. fs5 and fs7\n",
    "6. fs8 and fs9\n",
    "7. fs8 and fs10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answers using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A=?x, B=[C=?x, D='d']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs1.unify(fs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A='d', B=[C='d']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs1.unify(fs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A=(1)[B='b', D=?x, E->(1), F=?x], C->(1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs4.unify(fs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A=(1)[D='d'], C=[E->(1), F='d']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs5.unify(fs6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs5.unify(fs7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A=(1)[B='b', D='e', G='e'], C=[B='e', E->(1)]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs8.unify(fs9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A=(1)[B='b', D='b', E->(1), G='b'], C->(1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs8.unify(fs10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List two feature structures that subsume [A=?x, B=?x]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct('[A=?x, B=?x]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2 = nltk.FeatStruct('[A=?x]')\n",
    "fs3 = nltk.FeatStruct('[B=?x]')\n",
    "fs4 = nltk.FeatStruct('[A=?x, B=?x]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsumes(fs2, fs1), subsumes(fs3, fs1), subsumes(fs4, fs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring structure sharing, give an informal algorithm for unifying two feature structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the German grammar in 3.2 so that it can handle so-called verb-second structures like the following:\n",
    "Heute sieht der Hund die Katze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.FeatureGrammar.fromstring(\"\"\"\n",
    "    # Grammar Productions\n",
    "    S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
    "    S -> ADV V[SUBCAT=intrans, AGR=?a] NP[CASE=nom, AGR=?a]\n",
    "    S -> ADV V[SUBCAT=trans, OBJCASE=?c, AGR=?a] NP[CASE=nom, AGR=?a] NP[CASE=?c]\n",
    "    NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
    "    NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
    "    VP[AGR=?a] -> V[SUBCAT=intrans, AGR=?a]\n",
    "    VP[AGR=?a] -> V[SUBCAT=trans, OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
    "    VP[AGR=?a] -> V[SUBCAT=intrans, AGR=?a] ADV\n",
    "    VP[AGR=?a] -> V[SUBCAT=trans, OBJCASE=?c, AGR=?a] ADV NP[CASE=?c]\n",
    "    # Lexical Productions\n",
    "    # Singular determiners\n",
    "    # masc\n",
    "    Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n",
    "    Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
    "    Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
    "    # fem\n",
    "    Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    "    Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
    "    Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    "    # Plural determiners\n",
    "    Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    "    Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n",
    "    Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    "    # Nouns\n",
    "    N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
    "    N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "    N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
    "    N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    "    N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
    "    N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
    "    # Pronouns\n",
    "    PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
    "    PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
    "    PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
    "    PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
    "    PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
    "    PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
    "    PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "    PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    "    PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
    "    PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
    "    # Verbs\n",
    "    V[SUBCAT=intrans, AGR=[NUM=sg,PER=1]] -> 'komme'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
    "    V[SUBCAT=intrans, AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
    "    V[SUBCAT=trans, OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
    "    V[SUBCAT=trans, OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
    "    # Adverbs\n",
    "    ADV -> 'Heute' | 'heute'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Heute sieht der Hund die Katze'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.FeatureEarleyChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (ADV[] Heute)\n",
      "  (V[AGR=[NUM='sg', PER=3], OBJCASE='acc', SUBCAT='trans'] sieht)\n",
      "  (NP[AGR=[GND='masc', NUM='sg', PER=3], CASE='nom']\n",
      "    (Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='nom'] der)\n",
      "    (N[AGR=[GND='masc', NUM='sg', PER=3]] Hund))\n",
      "  (NP[AGR=[GND='fem', NUM='sg', PER=3], CASE='acc']\n",
      "    (Det[AGR=[GND='fem', NUM='sg', PER=3], CASE='acc'] die)\n",
      "    (N[AGR=[GND='fem', NUM='sg', PER=3]] Katze)))\n"
     ]
    }
   ],
   "source": [
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seemingly synonymous verbs have slightly different syntactic properties (Levin, 1993). Consider the patterns of grammaticality for the verbs loaded, filled, and dumped below. Can you write grammar productions to handle such data?\n",
    "\n",
    "(59)\t\n",
    "a. The farmer loaded the cart with sand\t\n",
    "b. The farmer loaded sand into the cart\t\n",
    "c. The farmer filled the cart with sand\t\n",
    "d. The farmer filled sand into the cart\t\n",
    "e. The farmer dumped the cart with sand\t\n",
    "f. The farmer dumped sand into the cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.FeatureGrammar.fromstring(\"\"\"\n",
    "    S -> NP VP PP\n",
    "    \n",
    "    VP -> V NP\n",
    "    PP -> Prep[COUNT=?c] NP[COUNT=?c]\n",
    "    NP[COUNT=?c] -> Det N[COUNT=?c] | N[COUNT=?c]\n",
    "    \n",
    "    V -> 'filled' | 'loaded' | 'dumped'\n",
    "    N[+COUNT] -> 'farmer' | 'cart'\n",
    "    N[-COUNT] -> 'sand'\n",
    "    Prep[+COUNT] -> 'into'\n",
    "    Prep[-COUNT] -> 'with'\n",
    "    Det -> 'The' | 'the'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'The farmer loaded the cart with sand'.split()\n",
    "sent2 = 'The farmer loaded sand into the cart'.split()\n",
    "sent3 = 'The farmer filled the cart with sand'.split()\n",
    "sent4 = 'The farmer filled sand into the cart'.split()\n",
    "sent5 = 'The farmer dumped the cart with sand'.split()\n",
    "sent6 = 'The farmer dumped sand into the cart'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.FeatureEarleyChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[+COUNT] (Det[] The) (N[+COUNT] farmer))\n",
      "  (VP[] (V[] loaded) (NP[+COUNT] (Det[] the) (N[+COUNT] cart)))\n",
      "  (PP[] (Prep[-COUNT] with) (NP[-COUNT] (N[-COUNT] sand))))\n",
      "(S[]\n",
      "  (NP[+COUNT] (Det[] The) (N[+COUNT] farmer))\n",
      "  (VP[] (V[] loaded) (NP[-COUNT] (N[-COUNT] sand)))\n",
      "  (PP[]\n",
      "    (Prep[+COUNT] into)\n",
      "    (NP[+COUNT] (Det[] the) (N[+COUNT] cart))))\n",
      "(S[]\n",
      "  (NP[+COUNT] (Det[] The) (N[+COUNT] farmer))\n",
      "  (VP[] (V[] filled) (NP[+COUNT] (Det[] the) (N[+COUNT] cart)))\n",
      "  (PP[] (Prep[-COUNT] with) (NP[-COUNT] (N[-COUNT] sand))))\n",
      "(S[]\n",
      "  (NP[+COUNT] (Det[] The) (N[+COUNT] farmer))\n",
      "  (VP[] (V[] filled) (NP[-COUNT] (N[-COUNT] sand)))\n",
      "  (PP[]\n",
      "    (Prep[+COUNT] into)\n",
      "    (NP[+COUNT] (Det[] the) (N[+COUNT] cart))))\n",
      "(S[]\n",
      "  (NP[+COUNT] (Det[] The) (N[+COUNT] farmer))\n",
      "  (VP[] (V[] dumped) (NP[+COUNT] (Det[] the) (N[+COUNT] cart)))\n",
      "  (PP[] (Prep[-COUNT] with) (NP[-COUNT] (N[-COUNT] sand))))\n",
      "(S[]\n",
      "  (NP[+COUNT] (Det[] The) (N[+COUNT] farmer))\n",
      "  (VP[] (V[] dumped) (NP[-COUNT] (N[-COUNT] sand)))\n",
      "  (PP[]\n",
      "    (Prep[+COUNT] into)\n",
      "    (NP[+COUNT] (Det[] the) (N[+COUNT] cart))))\n"
     ]
    }
   ],
   "source": [
    "for sent in [sent1, sent2, sent3, sent4, sent5, sent6]:\n",
    "    for tree in parser.parse(sent):\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological paradigms are rarely completely regular, in the sense of every cell in the matrix having a different realization. For example, the present tense conjugation of the lexeme walk only has two distinct forms: walks for the 3rd person singular, and walk for all other combinations of person and number. A successful analysis should not require redundantly specifying that 5 out of the 6 possible morphological combinations have the same realization. Propose and implement a method for dealing with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So-called head features are shared between the parent node and head child. For example, TENSE is a head feature that is shared between a VP and its head V child. See (Gazdar, Klein, & and, 1985) for more details. Most of the features we have looked at are head features — exceptions are SUBCAT and SLASH. Since the sharing of head features is predictable, it should not need to be stated explicitly in the grammar productions. Develop an approach that automatically accounts for this regular behavior of head features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend NLTK's treatment of feature structures to allow unification into list-valued features, and use this to implement an HPSG-style analysis of subcategorization, whereby the SUBCAT of a head category is the concatenation its complements' categories with the SUBCAT value of its immediate parent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend NLTK's treatment of feature structures to allow productions with underspecified categories, such as S[-INV] --> ?x S/?x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend NLTK's treatment of feature structures to allow typed feature structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick some grammatical constructions described in (Huddleston & Pullum, 2002), and develop a feature based grammar to account for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
