{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IOB format categorizes tagged tokens as I, O and B. Why are three tags necessary? What problem would be caused if we used I and O tags exclusively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot determine where chunks actually start because there will be no borders between adjacent chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_string = \"NP: {<(JJ|CD|DT).*>+<NNS?>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(chunk_string, tagged_sents):\n",
    "    chunk_label = chunk_string[:chunk_string.find(':')]\n",
    "    cp = nltk.RegexpParser(chunk_string)\n",
    "    for sent in tagged_sents:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == chunk_label: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP recent/JJ primary/NN)\n",
      "(NP any/DTI irregularities/NNS)\n",
      "(NP over-all/JJ charge/NN)\n",
      "(NP hard-fought/JJ primary/NN)\n",
      "(NP relative/JJ handful/NN)\n",
      "(NP such/JJ reports/NNS)\n",
      "(NP widespread/JJ interest/NN)\n",
      "(NP this/DT city/NN)\n",
      "(NP these/DTS laws/NNS)\n",
      "(NP grand/JJ jury/NN)\n",
      "(NP best/JJT interest/NN)\n",
      "(NP these/DTS two/CD offices/NNS)\n",
      "(NP greater/JJR efficiency/NN)\n",
      "(NP clerical/JJ personnel/NNS)\n",
      "(NP this/DT problem/NN)\n",
      "(NP outgoing/JJ jury/NN)\n",
      "(NP effective/JJ date/NN)\n",
      "(NP orderly/JJ implementation/NN)\n",
      "(NP grand/JJ jury/NN)\n",
      "(NP federal/JJ funds/NNS)\n",
      "(NP foster/JJ homes/NNS)\n",
      "(NP major/JJ items/NNS)\n",
      "(NP general/JJ assistance/NN)\n",
      "(NP these/DTS funds/NNS)\n",
      "(NP this/DT money/NN)\n",
      "(NP proportionate/JJ distribution/NN)\n",
      "(NP these/DTS funds/NNS)\n",
      "(NP this/DT program/NN)\n",
      "(NP populous/JJ counties/NNS)\n",
      "(NP some/DTI portion/NN)\n",
      "(NP these/DTS available/JJ funds/NNS)\n",
      "(NP disproportionate/JJ burden/NN)\n",
      "(NP two/CD previous/JJ grand/JJ juries/NNS)\n",
      "(NP These/DTS actions/NNS)\n",
      "(NP undue/JJ costs/NNS)\n",
      "(NP unmeritorious/JJ criticisms/NNS)\n",
      "(NP new/JJ multi-million-dollar/JJ airport/NN)\n",
      "(NP new/JJ management/NN)\n",
      "(NP political/JJ influences/NNS)\n",
      "(NP periodic/JJ surveillance/NN)\n",
      "(NP Four/CD additional/JJ deputies/NNS)\n",
      "(NP medical/JJ intern/NN)\n",
      "(NP mental/JJ cruelty/NN)\n",
      "(NP amicable/JJ property/NN)\n",
      "(NP one/CD brief/JJ interlude/NN)\n",
      "(NP political/JJ career/NN)\n",
      "(NP present/JJ term/NN)\n",
      "(NP 13/CD primary/NN)\n",
      "(NP strong/JJ encouragement/NN)\n",
      "(NP top/JJS official/NN)\n",
      "(NP enthusiastic/JJ responses/NNS)\n",
      "(NP unanimous/JJ vote/NN)\n"
     ]
    }
   ],
   "source": [
    "search_chunks(chunk_string, brown.tagged_sents()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the three chunk types in the CoNLL corpus. Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular expression chunker nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunk_type(chunked_sents, chunk_type):\n",
    "    for sent in chunked_sents:\n",
    "        for subtree in sent.subtrees():\n",
    "            if subtree.label() == chunk_type: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "(VP fail/VB to/TO show/VB)\n",
      "(VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "(VP reckon/VBP)\n",
      "(VP has/VBZ been/VBN eroded/VBN)\n",
      "(VP to/TO announce/VB)\n",
      "(VP has/VBZ increased/VBN)\n",
      "(VP being/VBG forced/VBN to/TO increase/VB)\n",
      "(VP to/TO defend/VB)\n",
      "(VP say/VBP)\n",
      "(VP are/VBP)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP could/MD be/VB)\n",
      "(VP noted/VBD)\n",
      "(VP range/VBP)\n",
      "(VP expect/VBP)\n",
      "(VP to/TO show/VB)\n",
      "(VP reported/VBD)\n",
      "(VP registered/VBN)\n",
      "(VP are/VBP topped/VBN)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP is/VBZ transforming/VBG)\n",
      "(VP to/TO boost/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP reckons/VBZ)\n",
      "(VP will/MD narrow/VB)\n",
      "(VP said/VBD)\n",
      "(VP believes/VBZ)\n",
      "(VP could/MD lead/VB)\n",
      "(VP could/MD narrow/VB)\n",
      "(VP forecasts/VBZ)\n",
      "(VP warns/VBZ)\n",
      "(VP are/VBP)\n",
      "(VP wo/MD n't/RB advance/VB)\n",
      "(VP will/MD want/VB to/TO see/VB)\n",
      "(VP adjusting/VBG)\n",
      "(VP noted/VBD)\n",
      "(VP will/MD want/VB to/TO go/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP warned/VBD)\n",
      "(VP can/MD be/VB expected/VBN)\n",
      "(VP takes/VBZ)\n",
      "(VP are/VBP)\n",
      "(VP released/VBD)\n",
      "(VP do/VBP n't/RB suggest/VB)\n",
      "(VP is/VBZ slowing/VBG)\n",
      "(VP show/VBP)\n",
      "(VP rose/VBD)\n",
      "(VP was/VBD)\n",
      "(VP compares/VBZ)\n",
      "(VP said/VBD)\n",
      "(VP show/VBP)\n",
      "(VP is/VBZ)\n",
      "(VP went/VBD)\n",
      "(VP should/MD reduce/VB)\n",
      "(VP has/VBZ made/VBN)\n",
      "(VP is/VBZ prepared/VBN to/TO increase/VB)\n",
      "(VP to/TO both/DT ensure/VB)\n",
      "(VP does/VBZ take/VB)\n",
      "(VP does/VBZ n't/RB decline/VB)\n",
      "(VP reminded/VBD)\n",
      "(VP can/MD not/RB allow/VB)\n",
      "(VP to/TO be/VB undermined/VBN)\n",
      "(VP agree/VBP)\n",
      "(VP is/VBZ)\n",
      "(VP holding/NN)\n",
      "(VP will/MD be/VB pushed/VBN)\n",
      "(VP warn/VBP)\n",
      "(VP could/MD swiftly/RB make/VB)\n",
      "(VP sound/NN)\n",
      "(VP was/VBD already/RB showing/VBG)\n",
      "(VP declined/VBD)\n",
      "(VP suggested/VBD)\n",
      "(VP falls/VBZ)\n",
      "(VP will/MD be/VB forced/VBN to/TO increase/VB)\n",
      "(VP both/DT to/TO)\n",
      "(VP halt/VB)\n",
      "(VP ensure/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP posted/VBD)\n",
      "(VP abated/VBN)\n",
      "(VP said/VBD)\n",
      "(VP has/VBZ begun/VBN to/TO distance/VB)\n",
      "(VP has/VBZ preoccupied/VBN)\n",
      "(VP plunged/VBD)\n",
      "(VP predict/VBP)\n",
      "(VP will/MD shift/VB)\n",
      "(VP keeping/VBG)\n",
      "(VP was/VBD quoted/VBN)\n",
      "(VP was/VBD also/RB changing/VBG)\n",
      "(VP opened/VBD)\n",
      "(VP settled/VBD)\n",
      "(VP was/VBD)\n",
      "(VP was/VBD quoted/VBN)\n",
      "(VP said/VBD)\n",
      "(VP proposed/VBD to/TO acquire/VB)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP attached/VBN)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP obtaining/VBG)\n",
      "(VP declined/VBD to/TO comment/VB)\n",
      "(VP values/VBZ)\n",
      "(VP has/VBZ)\n",
      "(VP closed/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP said/VBD)\n",
      "(VP boosted/VBD)\n",
      "(VP holds/VBZ)\n",
      "(VP bought/VBD)\n",
      "(VP control/NN)\n"
     ]
    }
   ],
   "source": [
    "search_chunk_type(conll2000.chunked_sents('train.txt')[:50], 'VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser('VP: {<MD>?<V.*>*<RB>?<TO>?<V.*>+}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.3%%\n",
      "    Precision:     82.6%%\n",
      "    Recall:        88.7%%\n",
      "    F-Measure:     85.6%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(conll2000.chunked_sents('test.txt', chunk_types=['VP'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An early definition of chunk was the material that occurs between chinks. Develop a chunker that starts by putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program. Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_string = \"\"\"\n",
    "    VP:\n",
    "      {<.*>+}\n",
    "      }<(JJ|NN|IN|CD|DT|\\W|CC|PRP|W).*>+{\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(chunk_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.2%%\n",
      "    Precision:     59.2%%\n",
      "    Recall:        79.5%%\n",
      "    F-Measure:     67.9%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(conll2000.chunked_sents('test.txt', chunk_types=['VP'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of VP chunking, building chunk parser using chinking is not a good idea according to performance and simplicity perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pattern(pattern, n):\n",
    "    label = pattern.split()[0][:-1]\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    for sent in brown.tagged_sents()[:n]:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == label: \n",
    "                print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_vbg_pattern = \"\"\"\n",
    "    NP_VBG:\n",
    "      {<DT><VBG><N.*>}\n",
    "      {<N.*><VBG><N.*>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP_VBG County/NN-TL purchasing/VBG departments/NNS)\n",
      "(NP_VBG Dallas/NP authorizing/VBG establishment/NN)\n",
      "(NP_VBG Galveston/NP authorizing/VBG establishment/NN)\n",
      "(NP_VBG school/NN teaching/VBG certificate/NN)\n",
      "(NP_VBG welfare/NN consulting/VBG firm/NN)\n",
      "(NP_VBG cent/NN starting/VBG Jan./NP)\n",
      "(NP_VBG days/NNS following/VBG discharge/NN)\n",
      "(NP_VBG Community/NN visiting/VBG nurse/NN)\n",
      "(NP_VBG law/NN providing/VBG grants/NNS)\n",
      "(NP_VBG laws/NNS regulating/VBG Sunday/NR)\n",
      "(NP_VBG ordinance/NN permitting/VBG motorists/NNS)\n",
      "(NP_VBG state/NN financing/VBG aid/NN)\n",
      "(NP_VBG 1920s/NNS following/VBG adoption/NN)\n",
      "(NP_VBG Administration's/NN$-TL housing/VBG bill/NN)\n",
      "(NP_VBG each/DT passing/VBG week/NN)\n",
      "(NP_VBG points/NNS bordering/VBG Lafayette/NP-TL)\n",
      "(NP_VBG another/DT vexing/VBG issue/NN)\n",
      "(NP_VBG problem/NN confronting/VBG Davis/NP)\n",
      "(NP_VBG vouchers/NNS certifying/VBG work/NN)\n",
      "(NP_VBG bill/NN raising/VBG fees/NNS)\n",
      "(NP_VBG dinner/NN honoring/VBG Sen./NN-TL)\n"
     ]
    }
   ],
   "source": [
    "test_pattern(np_vbg_pattern, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write one or more tag patterns to handle coordinated noun phrases, e.g. \"July/NNP and/CC August/NNP\", \"all/DT your/PRP$ managers/NNS and/CC supervisors/NNS\", \"company/NN courts/NNS and/CC adjudicators/NNS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_cc_pattern = \"\"\"\n",
    "    NP_CC:\n",
    "      {<N.*><CC><N.*>}\n",
    "      {<DT><P.*><N.*><CC><N.*>}\n",
    "      {<N.*><N.*><CC><N.*>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP_CC praise/NN and/CC thanks/NNS)\n",
      "(NP_CC registration/NN and/CC election/NN)\n",
      "(NP_CC Atlanta/NP and/CC Fulton/NP-TL)\n",
      "(NP_CC guardians/NNS and/CC administrators/NNS)\n",
      "(NP_CC fees/NNS and/CC compensation/NN)\n",
      "(NP_CC intern/NN or/CC extern/NN)\n",
      "(NP_CC night/NN and/CC weekend/NN)\n",
      "(NP_CC administration/NN and/CC operation/NN)\n",
      "(NP_CC Bellwood/NP and/CC Alpharetta/NP)\n",
      "(NP_CC man/NN and/CC wife/NN)\n",
      "(NP_CC principal/NN and/CC chairman/NN)\n",
      "(NP_CC Davis/NP and/CC Bush/NP)\n",
      "(NP_CC insurance/NN and/CC pipeline/NN)\n",
      "(NP_CC Harlingen/NP and/CC Howard/NP)\n",
      "(NP_CC Tarrant/NP and/CC El/NP)\n",
      "(NP_CC Berry/NP and/CC Joe/NP)\n",
      "(NP_CC gifts/NNS and/CC donations/NNS)\n",
      "(NP_CC stocks/NNS and/CC bonds/NNS)\n",
      "(NP_CC Legislature/NN-TL and/CC Congress/NP)\n",
      "(NP_CC Dallas/NP and/CC Fort/NN-TL)\n",
      "(NP_CC Dallas/NP and/CC Sen./NN-TL)\n",
      "(NP_CC Newton/NP and/CC Joe/NP)\n",
      "(NP_CC Dallas/NP and/CC Fort/NN-TL)\n",
      "(NP_CC math/NN or/CC English/NP)\n",
      "(NP_CC A/NN &/CC I/NN)\n",
      "(NP_CC College/NN-TL and/CC Massachusetts/NP-TL)\n",
      "(NP_CC teacher/NN and/CC principal/NN)\n",
      "(NP_CC skills/NNS and/CC discrimination/NN)\n",
      "(NP_CC causes/NNS and/CC prevention/NN)\n",
      "(NP_CC dependency/NN and/CC illegitimacy/NN)\n",
      "(NP_CC time/NN and/CC attention/NN)\n",
      "(NP_CC television/NN and/CC radio/NN)\n",
      "(NP_CC demands/NNS and/CC proposals/NNS)\n",
      "(NP_CC guilt/NN or/CC innocence/NN)\n",
      "(NP_CC security/NN or/CC railroad/NN)\n",
      "(NP_CC worker/NN and/CC employer/NN)\n",
      "(NP_CC security/NN and/CC railroad/NN)\n",
      "(NP_CC doctor/NN and/CC hospital/NN)\n",
      "(NP_CC medicine/NN and/CC dentistry/NN)\n",
      "(NP_CC points/NNS and/CC setbacks/NNS)\n",
      "(NP_CC distance/NN or/CC lack/NN)\n",
      "(NP_CC support/NN and/CC cooperation/NN)\n",
      "(NP_CC burglary/NN and/CC larceny/NN)\n",
      "(NP_CC sirens/NNS and/CC rescue/NN)\n",
      "(NP_CC labor/NN and/CC special-interest/NN)\n",
      "(NP_CC State/NN-TL and/CC Rhode/NP-TL)\n",
      "(NP_CC grocery/NN and/CC variety/NN)\n",
      "(NP_CC outlets/NNS and/CC department/NN)\n",
      "(NP_CC Liberals/NNS and/CC conservatives/NNS)\n",
      "(NP_CC machinist/NN and/CC toolmaker/NN)\n",
      "(NP_CC days/NNS and/CC Mr./NP)\n",
      "(NP_CC Martinelli/NP and/CC John/NP)\n",
      "(NP_CC Mitchell/NP and/CC Sen./NN-TL)\n",
      "(NP_CC applause/NN or/CC boos/NNS)\n",
      "(NP_CC sales/NNS or/CC state/NN)\n",
      "(NP_CC school/NN and/CC transportation/NN)\n",
      "(NP_CC estate/NN or/CC motor/NN)\n",
      "(NP_CC leaders/NNS and/CC campaign/NN)\n",
      "(NP_CC interest/NN or/CC charge/NN)\n",
      "(NP_CC forestry/NN and/CC employment/NN)\n",
      "(NP_CC warden/NN and/CC chief/NN)\n",
      "(NP_CC plows/NNS and/CC tractor/NN)\n",
      "(NP_CC Cross/NN-TL and/CC State/NN-TL)\n",
      "(NP_CC candidate/NN and/CC party/NN)\n",
      "(NP_CC business/NN and/CC industry/NN)\n",
      "(NP_CC Screvane/NP and/CC Abraham/NP)\n",
      "(NP_CC district/NN and/CC county/NN)\n",
      "(NP_CC rank/NN and/CC file/NN)\n",
      "(NP_CC Council/NN-TL and/CC Mr./NP)\n",
      "(NP_CC city/NN and/CC state/NN)\n",
      "(NP_CC conflict/NN or/CC opposition/NN)\n",
      "(NP_CC Queens/NP and/CC Thomas/NP)\n",
      "(NP_CC languages/NNS and/CC mathematics/NN)\n",
      "(NP_CC Delaney/NP and/CC Mr./NP)\n",
      "(NP_CC agriculture/NN and/CC community/NN)\n",
      "(NP_CC church/NN and/CC state/NN)\n",
      "(NP_CC means/NNS and/CC goals/NNS)\n",
      "(NP_CC funds/NNS and/CC Peace/NN-TL)\n",
      "(NP_CC church/NN and/CC state/NN)\n",
      "(NP_CC staff/NN and/CC operation/NN)\n",
      "(NP_CC understanding/NN and/CC relaxation/NN)\n",
      "(NP_CC school/NN and/CC Soule/NP)\n",
      "(NP_CC shrines/NNS and/CC monuments/NNS)\n",
      "(NP_CC seats/NNS and/CC vantage/NN)\n",
      "(NP_CC stand/NN and/CC Lafayette/NP-TL)\n",
      "(NP_CC Massachusetts/NP and/CC Texas/NP)\n",
      "(NP_CC job-seekers/NNS and/CC others/NNS)\n",
      "(NP_CC loyalists/NNS and/CC independents/NNS)\n",
      "(NP_CC Eastland/NP and/CC John/NP)\n",
      "(NP_CC peace/NN and/CC harmony/NN)\n",
      "(NP_CC architecture/NN and/CC engineering/NN)\n",
      "(NP_CC companies/NNS and/CC city/NN)\n",
      "(NP_CC Hemphill/NP and/CC District/NN-TL)\n",
      "(NP_CC permits/NNS and/CC city/NN)\n",
      "(NP_CC hotel/NN and/CC apartment/NN)\n",
      "(NP_CC males/NNS and/CC $2/NNS)\n",
      "(NP_CC catchers/NNS and/CC $15,000/NNS)\n",
      "(NP_CC Lieberman/NP and/CC Health/NN-TL)\n",
      "(NP_CC A./NP and/CC Henry/NP)\n",
      "(NP_CC TNT/NN or/CC nitroglycerine/NN)\n",
      "(NP_CC person/NN or/CC persons/NNS)\n",
      "(NP_CC Mining/NP and/CC Eugene/NP)\n",
      "(NP_CC local/NN and/CC Shiflett/NP)\n",
      "(NP_CC ritiuality/NN and/CC loyalty/NN)\n",
      "(NP_CC blueprints/NNS and/CC specifications/NNS)\n",
      "(NP_CC Goldberg/NP and/CC Sen./NN-TL)\n",
      "(NP_CC Green/NP and/CC Al/NP)\n",
      "(NP_CC Nilsen/NP and/CC Mayor/NN-TL)\n",
      "(NP_CC Bubenik/NP and/CC Frank/NP)\n",
      "(NP_CC Sexton/NP and/CC Theodore/NP)\n",
      "(NP_CC editing/NN and/CC clarification/NN)\n",
      "(NP_CC faith/NN and/CC conduct/NN)\n"
     ]
    }
   ],
   "source": [
    "test_pattern(np_cc_pattern, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will produce errors.)<div>\n",
    "    a. Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure.<div>\n",
    "    b. Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker. Discuss.<div>\n",
    "    c. Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter.<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
