{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IOB format categorizes tagged tokens as I, O and B. Why are three tags necessary? What problem would be caused if we used I and O tags exclusively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot determine where chunks actually start because there will be no borders between adjacent chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_string = \"NP: {<(JJ|CD|DT).*>+<NNS?>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(chunk_string, tagged_sents):\n",
    "    chunk_label = chunk_string[:chunk_string.find(':')]\n",
    "    cp = nltk.RegexpParser(chunk_string)\n",
    "    for sent in tagged_sents:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == chunk_label: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP recent/JJ primary/NN)\n",
      "(NP any/DTI irregularities/NNS)\n",
      "(NP over-all/JJ charge/NN)\n",
      "(NP hard-fought/JJ primary/NN)\n",
      "(NP relative/JJ handful/NN)\n",
      "(NP such/JJ reports/NNS)\n",
      "(NP widespread/JJ interest/NN)\n",
      "(NP this/DT city/NN)\n",
      "(NP these/DTS laws/NNS)\n",
      "(NP grand/JJ jury/NN)\n",
      "(NP best/JJT interest/NN)\n",
      "(NP these/DTS two/CD offices/NNS)\n",
      "(NP greater/JJR efficiency/NN)\n",
      "(NP clerical/JJ personnel/NNS)\n",
      "(NP this/DT problem/NN)\n",
      "(NP outgoing/JJ jury/NN)\n",
      "(NP effective/JJ date/NN)\n",
      "(NP orderly/JJ implementation/NN)\n",
      "(NP grand/JJ jury/NN)\n",
      "(NP federal/JJ funds/NNS)\n",
      "(NP foster/JJ homes/NNS)\n",
      "(NP major/JJ items/NNS)\n",
      "(NP general/JJ assistance/NN)\n",
      "(NP these/DTS funds/NNS)\n",
      "(NP this/DT money/NN)\n",
      "(NP proportionate/JJ distribution/NN)\n",
      "(NP these/DTS funds/NNS)\n",
      "(NP this/DT program/NN)\n",
      "(NP populous/JJ counties/NNS)\n",
      "(NP some/DTI portion/NN)\n",
      "(NP these/DTS available/JJ funds/NNS)\n",
      "(NP disproportionate/JJ burden/NN)\n",
      "(NP two/CD previous/JJ grand/JJ juries/NNS)\n",
      "(NP These/DTS actions/NNS)\n",
      "(NP undue/JJ costs/NNS)\n",
      "(NP unmeritorious/JJ criticisms/NNS)\n",
      "(NP new/JJ multi-million-dollar/JJ airport/NN)\n",
      "(NP new/JJ management/NN)\n",
      "(NP political/JJ influences/NNS)\n",
      "(NP periodic/JJ surveillance/NN)\n",
      "(NP Four/CD additional/JJ deputies/NNS)\n",
      "(NP medical/JJ intern/NN)\n",
      "(NP mental/JJ cruelty/NN)\n",
      "(NP amicable/JJ property/NN)\n",
      "(NP one/CD brief/JJ interlude/NN)\n",
      "(NP political/JJ career/NN)\n",
      "(NP present/JJ term/NN)\n",
      "(NP 13/CD primary/NN)\n",
      "(NP strong/JJ encouragement/NN)\n",
      "(NP top/JJS official/NN)\n",
      "(NP enthusiastic/JJ responses/NNS)\n",
      "(NP unanimous/JJ vote/NN)\n"
     ]
    }
   ],
   "source": [
    "search_chunks(chunk_string, brown.tagged_sents()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the three chunk types in the CoNLL corpus. Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular expression chunker nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunk_type(chunked_sents, chunk_type):\n",
    "    for sent in chunked_sents:\n",
    "        for subtree in sent.subtrees():\n",
    "            if subtree.label() == chunk_type: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "(VP fail/VB to/TO show/VB)\n",
      "(VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "(VP reckon/VBP)\n",
      "(VP has/VBZ been/VBN eroded/VBN)\n",
      "(VP to/TO announce/VB)\n",
      "(VP has/VBZ increased/VBN)\n",
      "(VP being/VBG forced/VBN to/TO increase/VB)\n",
      "(VP to/TO defend/VB)\n",
      "(VP say/VBP)\n",
      "(VP are/VBP)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP could/MD be/VB)\n",
      "(VP noted/VBD)\n",
      "(VP range/VBP)\n",
      "(VP expect/VBP)\n",
      "(VP to/TO show/VB)\n",
      "(VP reported/VBD)\n",
      "(VP registered/VBN)\n",
      "(VP are/VBP topped/VBN)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP is/VBZ transforming/VBG)\n",
      "(VP to/TO boost/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP reckons/VBZ)\n",
      "(VP will/MD narrow/VB)\n",
      "(VP said/VBD)\n",
      "(VP believes/VBZ)\n",
      "(VP could/MD lead/VB)\n",
      "(VP could/MD narrow/VB)\n",
      "(VP forecasts/VBZ)\n",
      "(VP warns/VBZ)\n",
      "(VP are/VBP)\n",
      "(VP wo/MD n't/RB advance/VB)\n",
      "(VP will/MD want/VB to/TO see/VB)\n",
      "(VP adjusting/VBG)\n",
      "(VP noted/VBD)\n",
      "(VP will/MD want/VB to/TO go/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP warned/VBD)\n",
      "(VP can/MD be/VB expected/VBN)\n",
      "(VP takes/VBZ)\n",
      "(VP are/VBP)\n",
      "(VP released/VBD)\n",
      "(VP do/VBP n't/RB suggest/VB)\n",
      "(VP is/VBZ slowing/VBG)\n",
      "(VP show/VBP)\n",
      "(VP rose/VBD)\n",
      "(VP was/VBD)\n",
      "(VP compares/VBZ)\n",
      "(VP said/VBD)\n",
      "(VP show/VBP)\n",
      "(VP is/VBZ)\n",
      "(VP went/VBD)\n",
      "(VP should/MD reduce/VB)\n",
      "(VP has/VBZ made/VBN)\n",
      "(VP is/VBZ prepared/VBN to/TO increase/VB)\n",
      "(VP to/TO both/DT ensure/VB)\n",
      "(VP does/VBZ take/VB)\n",
      "(VP does/VBZ n't/RB decline/VB)\n",
      "(VP reminded/VBD)\n",
      "(VP can/MD not/RB allow/VB)\n",
      "(VP to/TO be/VB undermined/VBN)\n",
      "(VP agree/VBP)\n",
      "(VP is/VBZ)\n",
      "(VP holding/NN)\n",
      "(VP will/MD be/VB pushed/VBN)\n",
      "(VP warn/VBP)\n",
      "(VP could/MD swiftly/RB make/VB)\n",
      "(VP sound/NN)\n",
      "(VP was/VBD already/RB showing/VBG)\n",
      "(VP declined/VBD)\n",
      "(VP suggested/VBD)\n",
      "(VP falls/VBZ)\n",
      "(VP will/MD be/VB forced/VBN to/TO increase/VB)\n",
      "(VP both/DT to/TO)\n",
      "(VP halt/VB)\n",
      "(VP ensure/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP posted/VBD)\n",
      "(VP abated/VBN)\n",
      "(VP said/VBD)\n",
      "(VP has/VBZ begun/VBN to/TO distance/VB)\n",
      "(VP has/VBZ preoccupied/VBN)\n",
      "(VP plunged/VBD)\n",
      "(VP predict/VBP)\n",
      "(VP will/MD shift/VB)\n",
      "(VP keeping/VBG)\n",
      "(VP was/VBD quoted/VBN)\n",
      "(VP was/VBD also/RB changing/VBG)\n",
      "(VP opened/VBD)\n",
      "(VP settled/VBD)\n",
      "(VP was/VBD)\n",
      "(VP was/VBD quoted/VBN)\n",
      "(VP said/VBD)\n",
      "(VP proposed/VBD to/TO acquire/VB)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP attached/VBN)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP obtaining/VBG)\n",
      "(VP declined/VBD to/TO comment/VB)\n",
      "(VP values/VBZ)\n",
      "(VP has/VBZ)\n",
      "(VP closed/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP said/VBD)\n",
      "(VP boosted/VBD)\n",
      "(VP holds/VBZ)\n",
      "(VP bought/VBD)\n",
      "(VP control/NN)\n"
     ]
    }
   ],
   "source": [
    "search_chunk_type(conll2000.chunked_sents('train.txt')[:50], 'VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser('VP: {<MD>?<V.*>*<RB>?<TO>?<V.*>+}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.3%%\n",
      "    Precision:     82.6%%\n",
      "    Recall:        88.7%%\n",
      "    F-Measure:     85.6%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(conll2000.chunked_sents('test.txt', chunk_types=['VP'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
