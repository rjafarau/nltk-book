{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IOB format categorizes tagged tokens as I, O and B. Why are three tags necessary? What problem would be caused if we used I and O tags exclusively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot determine where chunks actually start because there will be no borders between adjacent chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_string = \"NP: {<(JJ|CD|DT).*>+<NNS?>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(chunk_string, tagged_sents):\n",
    "    chunk_label = chunk_string[:chunk_string.find(':')]\n",
    "    cp = nltk.RegexpParser(chunk_string)\n",
    "    for sent in tagged_sents:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == chunk_label: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP recent/JJ primary/NN)\n",
      "(NP any/DTI irregularities/NNS)\n",
      "(NP over-all/JJ charge/NN)\n",
      "(NP hard-fought/JJ primary/NN)\n",
      "(NP relative/JJ handful/NN)\n",
      "(NP such/JJ reports/NNS)\n",
      "(NP widespread/JJ interest/NN)\n",
      "(NP this/DT city/NN)\n",
      "(NP these/DTS laws/NNS)\n",
      "(NP grand/JJ jury/NN)\n",
      "(NP best/JJT interest/NN)\n",
      "(NP these/DTS two/CD offices/NNS)\n",
      "(NP greater/JJR efficiency/NN)\n",
      "(NP clerical/JJ personnel/NNS)\n",
      "(NP this/DT problem/NN)\n",
      "(NP outgoing/JJ jury/NN)\n",
      "(NP effective/JJ date/NN)\n",
      "(NP orderly/JJ implementation/NN)\n",
      "(NP grand/JJ jury/NN)\n",
      "(NP federal/JJ funds/NNS)\n",
      "(NP foster/JJ homes/NNS)\n",
      "(NP major/JJ items/NNS)\n",
      "(NP general/JJ assistance/NN)\n",
      "(NP these/DTS funds/NNS)\n",
      "(NP this/DT money/NN)\n",
      "(NP proportionate/JJ distribution/NN)\n",
      "(NP these/DTS funds/NNS)\n",
      "(NP this/DT program/NN)\n",
      "(NP populous/JJ counties/NNS)\n",
      "(NP some/DTI portion/NN)\n",
      "(NP these/DTS available/JJ funds/NNS)\n",
      "(NP disproportionate/JJ burden/NN)\n",
      "(NP two/CD previous/JJ grand/JJ juries/NNS)\n",
      "(NP These/DTS actions/NNS)\n",
      "(NP undue/JJ costs/NNS)\n",
      "(NP unmeritorious/JJ criticisms/NNS)\n",
      "(NP new/JJ multi-million-dollar/JJ airport/NN)\n",
      "(NP new/JJ management/NN)\n",
      "(NP political/JJ influences/NNS)\n",
      "(NP periodic/JJ surveillance/NN)\n",
      "(NP Four/CD additional/JJ deputies/NNS)\n",
      "(NP medical/JJ intern/NN)\n",
      "(NP mental/JJ cruelty/NN)\n",
      "(NP amicable/JJ property/NN)\n",
      "(NP one/CD brief/JJ interlude/NN)\n",
      "(NP political/JJ career/NN)\n",
      "(NP present/JJ term/NN)\n",
      "(NP 13/CD primary/NN)\n",
      "(NP strong/JJ encouragement/NN)\n",
      "(NP top/JJS official/NN)\n",
      "(NP enthusiastic/JJ responses/NNS)\n",
      "(NP unanimous/JJ vote/NN)\n"
     ]
    }
   ],
   "source": [
    "search_chunks(chunk_string, brown.tagged_sents()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the three chunk types in the CoNLL corpus. Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk. Develop a simple chunker using the regular expression chunker nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunk_type(chunked_sents, chunk_type):\n",
    "    for sent in chunked_sents:\n",
    "        for subtree in sent.subtrees():\n",
    "            if subtree.label() == chunk_type: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "(VP fail/VB to/TO show/VB)\n",
      "(VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "(VP reckon/VBP)\n",
      "(VP has/VBZ been/VBN eroded/VBN)\n",
      "(VP to/TO announce/VB)\n",
      "(VP has/VBZ increased/VBN)\n",
      "(VP being/VBG forced/VBN to/TO increase/VB)\n",
      "(VP to/TO defend/VB)\n",
      "(VP say/VBP)\n",
      "(VP are/VBP)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP could/MD be/VB)\n",
      "(VP noted/VBD)\n",
      "(VP range/VBP)\n",
      "(VP expect/VBP)\n",
      "(VP to/TO show/VB)\n",
      "(VP reported/VBD)\n",
      "(VP registered/VBN)\n",
      "(VP are/VBP topped/VBN)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP is/VBZ transforming/VBG)\n",
      "(VP to/TO boost/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP reckons/VBZ)\n",
      "(VP will/MD narrow/VB)\n",
      "(VP said/VBD)\n",
      "(VP believes/VBZ)\n",
      "(VP could/MD lead/VB)\n",
      "(VP could/MD narrow/VB)\n",
      "(VP forecasts/VBZ)\n",
      "(VP warns/VBZ)\n",
      "(VP are/VBP)\n",
      "(VP wo/MD n't/RB advance/VB)\n",
      "(VP will/MD want/VB to/TO see/VB)\n",
      "(VP adjusting/VBG)\n",
      "(VP noted/VBD)\n",
      "(VP will/MD want/VB to/TO go/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP warned/VBD)\n",
      "(VP can/MD be/VB expected/VBN)\n",
      "(VP takes/VBZ)\n",
      "(VP are/VBP)\n",
      "(VP released/VBD)\n",
      "(VP do/VBP n't/RB suggest/VB)\n",
      "(VP is/VBZ slowing/VBG)\n",
      "(VP show/VBP)\n",
      "(VP rose/VBD)\n",
      "(VP was/VBD)\n",
      "(VP compares/VBZ)\n",
      "(VP said/VBD)\n",
      "(VP show/VBP)\n",
      "(VP is/VBZ)\n",
      "(VP went/VBD)\n",
      "(VP should/MD reduce/VB)\n",
      "(VP has/VBZ made/VBN)\n",
      "(VP is/VBZ prepared/VBN to/TO increase/VB)\n",
      "(VP to/TO both/DT ensure/VB)\n",
      "(VP does/VBZ take/VB)\n",
      "(VP does/VBZ n't/RB decline/VB)\n",
      "(VP reminded/VBD)\n",
      "(VP can/MD not/RB allow/VB)\n",
      "(VP to/TO be/VB undermined/VBN)\n",
      "(VP agree/VBP)\n",
      "(VP is/VBZ)\n",
      "(VP holding/NN)\n",
      "(VP will/MD be/VB pushed/VBN)\n",
      "(VP warn/VBP)\n",
      "(VP could/MD swiftly/RB make/VB)\n",
      "(VP sound/NN)\n",
      "(VP was/VBD already/RB showing/VBG)\n",
      "(VP declined/VBD)\n",
      "(VP suggested/VBD)\n",
      "(VP falls/VBZ)\n",
      "(VP will/MD be/VB forced/VBN to/TO increase/VB)\n",
      "(VP both/DT to/TO)\n",
      "(VP halt/VB)\n",
      "(VP ensure/VB)\n",
      "(VP remains/VBZ)\n",
      "(VP posted/VBD)\n",
      "(VP abated/VBN)\n",
      "(VP said/VBD)\n",
      "(VP has/VBZ begun/VBN to/TO distance/VB)\n",
      "(VP has/VBZ preoccupied/VBN)\n",
      "(VP plunged/VBD)\n",
      "(VP predict/VBP)\n",
      "(VP will/MD shift/VB)\n",
      "(VP keeping/VBG)\n",
      "(VP was/VBD quoted/VBN)\n",
      "(VP was/VBD also/RB changing/VBG)\n",
      "(VP opened/VBD)\n",
      "(VP settled/VBD)\n",
      "(VP was/VBD)\n",
      "(VP was/VBD quoted/VBN)\n",
      "(VP said/VBD)\n",
      "(VP proposed/VBD to/TO acquire/VB)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP attached/VBN)\n",
      "(VP said/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP obtaining/VBG)\n",
      "(VP declined/VBD to/TO comment/VB)\n",
      "(VP values/VBZ)\n",
      "(VP has/VBZ)\n",
      "(VP closed/VBD)\n",
      "(VP is/VBZ)\n",
      "(VP said/VBD)\n",
      "(VP boosted/VBD)\n",
      "(VP holds/VBZ)\n",
      "(VP bought/VBD)\n",
      "(VP control/NN)\n"
     ]
    }
   ],
   "source": [
    "search_chunk_type(conll2000.chunked_sents('train.txt')[:50], 'VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser('VP: {<MD>?<V.*>*<RB>?<TO>?<V.*>+}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.3%%\n",
      "    Precision:     82.6%%\n",
      "    Recall:        88.7%%\n",
      "    F-Measure:     85.6%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(conll2000.chunked_sents('test.txt', chunk_types=['VP'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An early definition of chunk was the material that occurs between chinks. Develop a chunker that starts by putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program. Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_string = \"\"\"\n",
    "    VP:\n",
    "      {<.*>+}\n",
    "      }<(JJ|NN|IN|CD|DT|\\W|CC|PRP|W).*>+{\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(chunk_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.2%%\n",
      "    Precision:     59.2%%\n",
      "    Recall:        79.5%%\n",
      "    F-Measure:     67.9%%\n"
     ]
    }
   ],
   "source": [
    "print(cp.evaluate(conll2000.chunked_sents('test.txt', chunk_types=['VP'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of VP chunking, building chunk parser using chinking is not a good idea according to performance and simplicity perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = [\n",
    "    nltk.Tree('S', [subtree.leaves() if 'VBG' in subtree else subtree for subtree in sent.subtrees()]) \n",
    "    for sent in conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = conll2000.chunked_sents('train.txt', chunk_types=['NP'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [subtree for subtree in x.subtrees(filter=lambda x: 'S')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Confidence', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('pound', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('widely', 'RB'),\n",
       " ('expected', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('take', 'VB'),\n",
       " ('another', 'DT'),\n",
       " ('sharp', 'JJ'),\n",
       " ('dive', 'NN'),\n",
       " ('if', 'IN'),\n",
       " ('trade', 'NN'),\n",
       " ('figures', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('September', 'NNP'),\n",
       " (',', ','),\n",
       " ('due', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " ('release', 'NN'),\n",
       " ('tomorrow', 'NN'),\n",
       " (',', ','),\n",
       " ('fail', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('show', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('substantial', 'JJ'),\n",
       " ('improvement', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('July', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('August', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('near-record', 'JJ'),\n",
       " ('deficits', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NP', [('Confidence', 'NN')]),\n",
       " ('in', 'IN'),\n",
       " Tree('NP', [('the', 'DT'), ('pound', 'NN')]),\n",
       " ('is', 'VBZ'),\n",
       " ('widely', 'RB'),\n",
       " ('expected', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('take', 'VB'),\n",
       " Tree('NP', [('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN')]),\n",
       " ('if', 'IN'),\n",
       " Tree('NP', [('trade', 'NN'), ('figures', 'NNS')]),\n",
       " ('for', 'IN'),\n",
       " Tree('NP', [('September', 'NNP')]),\n",
       " (',', ','),\n",
       " ('due', 'JJ'),\n",
       " ('for', 'IN'),\n",
       " Tree('NP', [('release', 'NN')]),\n",
       " Tree('NP', [('tomorrow', 'NN')]),\n",
       " (',', ','),\n",
       " ('fail', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('show', 'VB'),\n",
       " Tree('NP', [('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN')]),\n",
       " ('from', 'IN'),\n",
       " Tree('NP', [('July', 'NNP'), ('and', 'CC'), ('August', 'NNP')]),\n",
       " Tree('NP', [(\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS')]),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y.flatten() if isinstance(y, nltk.Tree) and 'VBG' in {t for w, t in y.leaves()} else y for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'RB',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VB',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'CC',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-2bfb98fe2801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "x.subtrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
