{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 5.1 the new field appeared at the bottom of the entry. Modify this program so that it inserts the new subelement right after the lx field. (Hint: create the new cv field using Element('cv'), assign a text value to it, then use the insert() method of the parent element.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import SubElement, Element\n",
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z]', r'_', s)\n",
    "    s = re.sub(r'[aeiou]', r'V', s)\n",
    "    s = re.sub(r'[^V_]', r'C', s)\n",
    "    return (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cv_field(entry):\n",
    "    for i, field in enumerate(entry):\n",
    "        if field.tag == 'lx':\n",
    "            cv_field = Element('cv')\n",
    "            cv_field.text = cv(field.text)\n",
    "            entry.insert(i + 1, cv_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lx kaeviro\n",
      "\\cv CVVCVCV\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\ge lift off\n",
      "\\ge take off\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')\n",
    "add_cv_field(lexicon[53])\n",
    "print(nltk.toolbox.to_sfm_string(lexicon[53]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that deletes a specified field from a lexical entry. (We could use this to sanitize our lexical data before giving it to others, e.g. by removing fields containing irrelevant or uncertain content.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "from xml.etree.ElementTree import SubElement, Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_field(entry, field_tag):\n",
    "    for field in entry.findall(field_tag):\n",
    "        entry.remove(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lx kaeviro\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entry = lexicon[53]\n",
    "remove_field(entry, 'ge')\n",
    "print(nltk.toolbox.to_sfm_string(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program that scans an HTML dictionary file to find entries having an illegal part-of-speech field, and reports the headword for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_illegal_pos(lexicon, valid_pos=['N', 'V', 'ADV']):\n",
    "    headwords = []\n",
    "    for entry in lexicon:\n",
    "        ps = entry.find('ps')\n",
    "        if isinstance(ps, Element) and ps.text not in valid_pos:\n",
    "            headwords.append(entry.find('lx').text)\n",
    "    return headwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaakaavo',\n",
       " 'kaapea',\n",
       " 'kaapo',\n",
       " 'kaare',\n",
       " 'kaekae',\n",
       " 'kaereasi',\n",
       " 'kaetu',\n",
       " 'kakae',\n",
       " 'kakae',\n",
       " 'kakapikoa']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_illegal_pos(lexicon)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to find any parts of speech (ps field) that occurred less than ten times. Perhaps these are typing mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rare_pos(lexicon, n=10):\n",
    "    fd = nltk.FreqDist(\n",
    "        field.text\n",
    "        for entry in lexicon\n",
    "        for field in entry\n",
    "        if field.tag == 'ps'\n",
    "    )\n",
    "    return [t for t, c in fd.items() if c < n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLASS', 'FFP', 'NUM', 'POST', 'EXCL']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_rare_pos(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw a method for discovering cases of whole-word reduplication. Write a function to find words that may contain partial reduplication. Use the re.search() method, and the following regular expression: (..+)\\1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_reduplication(words):\n",
    "    return [w for w in words if re.search(r'(..+)\\1', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ablatitious',\n",
       " 'abstractitious',\n",
       " 'acacatechin',\n",
       " 'acacatechol',\n",
       " 'acacetin',\n",
       " 'acaciin',\n",
       " 'acacin',\n",
       " 'acarodermatitis',\n",
       " 'acatastasia',\n",
       " 'acclimatation']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_reduplication_words(words.words())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw a method for adding a cv field. There is an interesting issue with keeping this up-to-date when someone modifies the content of the lx field on which it is based. Write a version of this program to add a cv field, replacing any existing cv field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import SubElement\n",
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z]', r'_', s)\n",
    "    s = re.sub(r'[aeiou]', r'V', s)\n",
    "    s = re.sub(r'[^V_]', r'C', s)\n",
    "    return (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cv_field(entry):\n",
    "    remove_field(entry, 'cv')\n",
    "    for i, field in enumerate(entry):\n",
    "        if field.tag == 'lx':\n",
    "            cv_field = Element('cv')\n",
    "            cv_field.text = cv(field.text)\n",
    "            entry.insert(i + 1, cv_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lx kaeviro\n",
      "\\cv CVVCVCV\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\ge lift off\n",
      "\\ge take off\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entry = lexicon[53]\n",
    "add_cv_field(entry)\n",
    "print(nltk.toolbox.to_sfm_string(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to add a new field syl which gives a count of the number of syllables in the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nsyl(w):\n",
    "    return [len([s for s in pron if s[-1].isdigit()]) \n",
    "            for pron in cmu[w.lower()]]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_syl_field(entry):\n",
    "    remove_field(entry, 'syl')\n",
    "    for i, field in enumerate(entry):\n",
    "        if field.tag == 'lx':\n",
    "            cv_field = Element('nsyl')\n",
    "            cv_field.text = nsyl(field.text)\n",
    "            entry.insert(i + 1, cv_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which displays the complete entry for a lexeme. When the lexeme is incorrectly spelled it should display the entry for the most similarly spelled lexeme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a lexicon and finds which pairs of consecutive fields are most frequent (e.g. ps is often followed by pt). (This might help us to discover some of the structure of a lexical entry.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_tag_pairs(lexicon, n=10):\n",
    "    return nltk.FreqDist(\n",
    "        (f1.tag, f2.tag)\n",
    "        for entry in lexicon\n",
    "        for f1, f2 in nltk.bigrams(entry)\n",
    "    ).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ex', 'xp'), 1532),\n",
       " (('xp', 'xe'), 1526),\n",
       " (('ps', 'pt'), 835),\n",
       " (('ge', 'tkp'), 824),\n",
       " (('pt', 'ge'), 766),\n",
       " (('dt', 'ex'), 758),\n",
       " (('xe', 'ex'), 708),\n",
       " (('lx', 'ps'), 520),\n",
       " (('rt', 'ps'), 356),\n",
       " (('tkp', 'dt'), 327)]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_freq_tag_pairs(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spreadsheet using office software, containing one lexical entry per row, consisting of a headword, a part of speech, and a gloss. Save the spreadsheet in CSV format. Write Python code to read the CSV file and print it in Toolbox format, using lx for the headword, ps for the part of speech, and gl for the gloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "from xml.etree.ElementTree import Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ex10.csv\n"
     ]
    }
   ],
   "source": [
    "%%file ex10.csv\n",
    "kaa,V,gag\n",
    "kaakaaro,N,mixture\n",
    "kaakaaviko,N,type of beetle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2tlbx(path):\n",
    "    df = pd.read_csv(path, names=['lx', 'ps', 'gl'])\n",
    "    dictionary = Element('dictionary')\n",
    "    for _, row in df.iterrows():\n",
    "        record = Element('record')\n",
    "        for item, value in row.iteritems():\n",
    "            field = Element(item)\n",
    "            field.text = value\n",
    "            record.append(field)\n",
    "        dictionary.append(record)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = csv2tlbx('ex10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lx kaa\n",
      "\\ps V\n",
      "\\gl gag\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.toolbox.to_sfm_string(dictionary[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ex10.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the words of Shakespeare's plays, with the help of nltk.Index. The resulting data structure should permit lookup on individual words such as music, returning a list of references to acts, scenes and speeches, of the form [(3, 2, 9), (5, 1, 23), ...], where (3, 2, 9) indicates Act 3 Scene 2 Speech 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "from xml.etree.ElementTree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
    "merchant = ElementTree().parse(merchant_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_index = nltk.Index(\n",
    "    (word.lower(), (i, j, k))\n",
    "    for i, act in enumerate(merchant.findall('ACT'), 1)\n",
    "    for j, scene in enumerate(act.findall('SCENE'), 1)\n",
    "    for k, speech in enumerate(scene.findall('SPEECH'), 1)\n",
    "    for line in speech.findall('LINE') if line.text\n",
    "    for word in nltk.word_tokenize(line.text) if word.isalpha()\n",
    ")\n",
    "merchant_index = {k: sorted(set(v)) \n",
    "                  for k, v in merchant_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 2, 9), (5, 1, 23), (5, 1, 24), (5, 1, 25), (5, 1, 28), (5, 1, 29)]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_index['music']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a conditional frequency distribution which records the word length for each speech in The Merchant of Venice, conditioned on the name of the character, e.g. cfd['PORTIA'][12] would give us the number of speeches by Portia consisting of 12 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "from xml.etree.ElementTree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
    "merchant = ElementTree().parse(merchant_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (speech.find('SPEAKER').text.upper(),\n",
    "     len([word \n",
    "         for word in nltk.word_tokenize(line.text) \n",
    "         if word.isalpha()]))\n",
    "    for act in merchant.findall('ACT')\n",
    "    for scene in act.findall('SCENE')\n",
    "    for speech in scene.findall('SPEECH')\n",
    "    for line in speech.findall('LINE') if line.text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['PORTIA'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a comparative wordlist in CSV format, and write a program that prints those cognates having an edit-distance of at least three from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an index of those lexemes which appear in example sentences. Suppose the lexeme for a given entry is w. Then add a single cross-reference field xrf to this entry, referencing the headwords of other entries having example sentences containing w. Do this for all entries and save the result as a toolbox-format file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a recursive function to produce an XML representation for a tree, with non-terminals represented as XML elements, and leaves represented as text content, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
