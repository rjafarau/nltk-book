{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit\n",
    "Steven Bird, mEwan Klein, and Edward Loper\n",
    "http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Processing Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Accessing Text from the Web and from Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electronic Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = request.urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = response.read().decode('utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)\n",
    "# <class 'str'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176966"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)\n",
    "# 1176893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:75]\n",
    "# 'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)\n",
    "# <class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257727"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)\n",
    "# 254354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]\n",
    "# ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)\n",
    "# <class 'nltk.text.Text'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'exceptionally',\n",
       " 'hot',\n",
       " 'evening',\n",
       " 'early',\n",
       " 'in',\n",
       " 'July',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'came',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'garret',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " 'lodged',\n",
       " 'in',\n",
       " 'S.',\n",
       " 'Place',\n",
       " 'and',\n",
       " 'walked',\n",
       " 'slowly',\n",
       " ',',\n",
       " 'as',\n",
       " 'though',\n",
       " 'in',\n",
       " 'hesitation',\n",
       " ',',\n",
       " 'towards',\n",
       " 'K.',\n",
       " 'bridge',\n",
       " '.',\n",
       " 'He',\n",
       " 'had',\n",
       " 'successfully']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1024:1062]\n",
    "# ['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',\n",
    "#  'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',\n",
    "#  'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly',\n",
    "#  ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Katerina Ivanovna',\n",
       " 'Pyotr Petrovitch',\n",
       " 'Pulcheria Alexandrovna',\n",
       " 'Avdotya Romanovna',\n",
       " 'Rodion Romanovitch',\n",
       " 'Marfa Petrovna',\n",
       " 'Sofya Semyonovna',\n",
       " 'old woman',\n",
       " 'Project Gutenberg-tm',\n",
       " 'Porfiry Petrovitch',\n",
       " 'Amalia Ivanovna',\n",
       " 'great deal',\n",
       " 'young man',\n",
       " 'Nikodim Fomitch',\n",
       " 'Ilya Petrovitch',\n",
       " 'Project Gutenberg',\n",
       " 'Andrey Semyonovitch',\n",
       " 'Hay Market',\n",
       " 'Dmitri Prokofitch',\n",
       " 'Good heavens']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collocation_list()\n",
    "# Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
    "# Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
    "# woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
    "# great deal; Nikodim Fomitch; young man; Ilya Petrovitch; n't know;\n",
    "# Project Gutenberg; Dmitri Prokofitch; Andrey Semyonovitch; Hay Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5335"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")\n",
    "# 5338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157811"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.rfind(\"End of Project Gutenberg’s Crime\")\n",
    "# 1157743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw[5335:1157811]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")\n",
    "# 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = request.urlopen(url).read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:60]\n",
    "# '<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = nltk.clean_html(html)\n",
    "# NotImplementedError: To remove HTML markup, use BeautifulSoup's get_text() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = BeautifulSoup(html).get_text()\n",
    "raw = BeautifulSoup(html, \"lxml\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC',\n",
       " 'NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'NEWS',\n",
       " 'SPORT',\n",
       " 'WEATHER',\n",
       " 'WORLD',\n",
       " 'SERVICE',\n",
       " 'A-Z',\n",
       " 'INDEX',\n",
       " 'SEARCH',\n",
       " 'You',\n",
       " 'are',\n",
       " 'in',\n",
       " ':',\n",
       " 'Health',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " 'Africa',\n",
       " 'Americas',\n",
       " 'Asia-Pacific',\n",
       " 'Europe',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'UK',\n",
       " 'Business',\n",
       " 'Entertainment',\n",
       " 'Science/Nature',\n",
       " 'Technology',\n",
       " 'Health',\n",
       " 'Medical',\n",
       " 'notes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'SERVICES',\n",
       " 'Daily',\n",
       " 'E-mail',\n",
       " 'News',\n",
       " 'Ticker',\n",
       " 'Mobile/PDAs',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Text',\n",
       " 'Only',\n",
       " 'Feedback',\n",
       " 'Help',\n",
       " 'EDITIONS',\n",
       " 'Change',\n",
       " 'to',\n",
       " 'UK',\n",
       " 'Friday',\n",
       " ',',\n",
       " '27',\n",
       " 'September',\n",
       " ',',\n",
       " '2002',\n",
       " ',',\n",
       " '11:51',\n",
       " 'GMT',\n",
       " '12:51',\n",
       " 'UK',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'Scientists',\n",
       " 'believe',\n",
       " 'the',\n",
       " 'last',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'Finland',\n",
       " 'The',\n",
       " 'last',\n",
       " 'natural',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'die',\n",
       " 'out',\n",
       " 'within',\n",
       " '200',\n",
       " 'years',\n",
       " ',',\n",
       " 'scientists',\n",
       " 'believe',\n",
       " '.',\n",
       " 'A',\n",
       " 'study',\n",
       " 'by',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'Germany',\n",
       " 'suggests',\n",
       " 'people',\n",
       " 'with',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'are',\n",
       " 'an',\n",
       " 'endangered',\n",
       " 'species',\n",
       " 'and',\n",
       " 'will',\n",
       " 'become',\n",
       " 'extinct',\n",
       " 'by',\n",
       " '2202',\n",
       " '.',\n",
       " 'Researchers',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'last',\n",
       " 'truly',\n",
       " 'natural',\n",
       " 'blonde',\n",
       " 'will',\n",
       " 'be',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Finland',\n",
       " '-',\n",
       " 'the',\n",
       " 'country',\n",
       " 'with',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'proportion',\n",
       " 'of',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " 'Prof',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'But',\n",
       " 'they',\n",
       " 'say',\n",
       " 'too',\n",
       " 'few',\n",
       " 'people',\n",
       " 'now',\n",
       " 'carry',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'for',\n",
       " 'blondes',\n",
       " 'to',\n",
       " 'last',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'next',\n",
       " 'two',\n",
       " 'centuries',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'is',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'a',\n",
       " 'recessive',\n",
       " 'gene',\n",
       " '.',\n",
       " 'In',\n",
       " 'order',\n",
       " 'for',\n",
       " 'a',\n",
       " 'child',\n",
       " 'to',\n",
       " 'have',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " ',',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'on',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'of',\n",
       " 'the',\n",
       " 'family',\n",
       " 'in',\n",
       " 'the',\n",
       " 'grandparents',\n",
       " \"'\",\n",
       " 'generation',\n",
       " '.',\n",
       " 'Dyed',\n",
       " 'rivals',\n",
       " 'The',\n",
       " 'researchers',\n",
       " 'also',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'so-called',\n",
       " 'bottle',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'for',\n",
       " 'the',\n",
       " 'demise',\n",
       " 'of',\n",
       " 'their',\n",
       " 'natural',\n",
       " 'rivals',\n",
       " '.',\n",
       " 'They',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'dyed-blondes',\n",
       " 'are',\n",
       " 'more',\n",
       " 'attractive',\n",
       " 'to',\n",
       " 'men',\n",
       " 'who',\n",
       " 'choose',\n",
       " 'them',\n",
       " 'as',\n",
       " 'partners',\n",
       " 'over',\n",
       " 'true',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'Bottle-blondes',\n",
       " 'like',\n",
       " 'Ann',\n",
       " 'Widdecombe',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'But',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'dermatology',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unlikely',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'die',\n",
       " 'out',\n",
       " 'completely',\n",
       " '.',\n",
       " '``',\n",
       " 'Genes',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'unless',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'of',\n",
       " 'having',\n",
       " 'that',\n",
       " 'gene',\n",
       " 'or',\n",
       " 'by',\n",
       " 'chance',\n",
       " '.',\n",
       " 'They',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'told',\n",
       " 'BBC',\n",
       " 'News',\n",
       " 'Online',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'only',\n",
       " 'reason',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'disappear',\n",
       " 'is',\n",
       " 'if',\n",
       " 'having',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'was',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'case',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " '.',\n",
       " \"''\",\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " '28',\n",
       " 'Mar',\n",
       " '01',\n",
       " '|',\n",
       " 'Education',\n",
       " 'What',\n",
       " 'is',\n",
       " 'it',\n",
       " 'about',\n",
       " 'blondes',\n",
       " '?',\n",
       " '09',\n",
       " 'Apr',\n",
       " '99',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Platinum',\n",
       " 'blondes',\n",
       " 'are',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'dumb',\n",
       " '17',\n",
       " 'Apr',\n",
       " '02',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Hair',\n",
       " 'dye',\n",
       " 'cancer',\n",
       " 'alert',\n",
       " 'Internet',\n",
       " 'links',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'The',\n",
       " 'BBC',\n",
       " 'is',\n",
       " 'not',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'external',\n",
       " 'internet',\n",
       " 'sites',\n",
       " 'Top',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'now',\n",
       " ':',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'at',\n",
       " 'the',\n",
       " 'foot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'page',\n",
       " '.',\n",
       " 'E-mail',\n",
       " 'this',\n",
       " 'story',\n",
       " 'to',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'In',\n",
       " 'This',\n",
       " 'Section',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'How',\n",
       " 'sperm',\n",
       " 'wriggle',\n",
       " 'Bollywood',\n",
       " 'told',\n",
       " 'to',\n",
       " 'stub',\n",
       " 'it',\n",
       " 'out',\n",
       " 'Fears',\n",
       " 'over',\n",
       " 'tuna',\n",
       " 'health',\n",
       " 'risk',\n",
       " 'to',\n",
       " 'babies',\n",
       " 'Public',\n",
       " 'can',\n",
       " 'be',\n",
       " 'taught',\n",
       " 'to',\n",
       " 'spot',\n",
       " 'strokes',\n",
       " '^^',\n",
       " 'Back',\n",
       " 'to',\n",
       " 'top',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " '|',\n",
       " 'Africa',\n",
       " '|',\n",
       " 'Americas',\n",
       " '|',\n",
       " 'Asia-Pacific',\n",
       " '|',\n",
       " 'Europe',\n",
       " '|',\n",
       " 'Middle',\n",
       " 'East',\n",
       " '|',\n",
       " 'South',\n",
       " 'Asia',\n",
       " '|',\n",
       " 'UK',\n",
       " '|',\n",
       " 'Business',\n",
       " '|',\n",
       " 'Entertainment',\n",
       " '|',\n",
       " 'Science/Nature',\n",
       " '|',\n",
       " 'Technology',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '|',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " '|',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '|',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Sport',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Weather',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'World',\n",
       " 'Service',\n",
       " '>',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '©',\n",
       " 'MMIII',\n",
       " '|',\n",
       " 'News',\n",
       " 'Sources',\n",
       " '|',\n",
       " 'Privacy',\n",
       " '<',\n",
       " '!',\n",
       " '--',\n",
       " 'var',\n",
       " 'pCid=',\n",
       " \"''\",\n",
       " 'uk_bbc_0',\n",
       " \"''\",\n",
       " ';',\n",
       " 'var',\n",
       " 'w0=1',\n",
       " ';',\n",
       " 'var',\n",
       " 'refR=escape',\n",
       " '(',\n",
       " 'document.referrer',\n",
       " ')',\n",
       " ';',\n",
       " 'if',\n",
       " '(',\n",
       " 'refR.length',\n",
       " '>',\n",
       " '=252',\n",
       " ')',\n",
       " 'refR=refR.substring',\n",
       " '(',\n",
       " '0,252',\n",
       " ')',\n",
       " '+',\n",
       " \"''\",\n",
       " '...',\n",
       " \"''\",\n",
       " ';',\n",
       " '//',\n",
       " '--',\n",
       " '>',\n",
       " '<',\n",
       " '!',\n",
       " '--',\n",
       " 'var',\n",
       " 'w0=0',\n",
       " ';',\n",
       " '//',\n",
       " '--',\n",
       " '>',\n",
       " '<',\n",
       " '!',\n",
       " '--',\n",
       " 'if',\n",
       " '(',\n",
       " 'w0',\n",
       " ')',\n",
       " '{',\n",
       " 'var',\n",
       " 'imgN=',\n",
       " \"'\",\n",
       " '<',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//server-uk.imrworldwide.com/cgi-bin/count',\n",
       " '?',\n",
       " \"ref='+\",\n",
       " 'refR+',\n",
       " \"'\",\n",
       " '&',\n",
       " \"cid='+pCid+\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " 'width=1',\n",
       " 'height=1',\n",
       " '>',\n",
       " \"'\",\n",
       " ';',\n",
       " 'if',\n",
       " '(',\n",
       " 'navigator.userAgent.indexOf',\n",
       " '(',\n",
       " \"'Mac\",\n",
       " \"'\",\n",
       " ')',\n",
       " '!',\n",
       " '=-1',\n",
       " ')',\n",
       " '{',\n",
       " 'document.write',\n",
       " '(',\n",
       " 'imgN',\n",
       " ')',\n",
       " ';',\n",
       " '}',\n",
       " 'else',\n",
       " '{',\n",
       " 'document.write',\n",
       " '(',\n",
       " \"'\",\n",
       " '<',\n",
       " 'applet',\n",
       " 'code=',\n",
       " \"''\",\n",
       " 'Measure.class',\n",
       " \"''\",\n",
       " \"'+\",\n",
       " \"'codebase=\",\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//server-uk.imrworldwide.com/',\n",
       " \"''\",\n",
       " \"'+'width=1\",\n",
       " 'height=2',\n",
       " '>',\n",
       " \"'+\",\n",
       " \"'\",\n",
       " '<',\n",
       " 'param',\n",
       " 'name=',\n",
       " \"''\",\n",
       " 'ref',\n",
       " \"''\",\n",
       " 'value=',\n",
       " \"''\",\n",
       " \"'+refR+\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " '>',\n",
       " \"'+\",\n",
       " \"'\",\n",
       " '<',\n",
       " 'param',\n",
       " 'name=',\n",
       " \"''\",\n",
       " 'cid',\n",
       " \"''\",\n",
       " 'value=',\n",
       " \"''\",\n",
       " \"'+pCid+\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'textflow',\n",
       " '>',\n",
       " \"'+imgN+\",\n",
       " \"'\",\n",
       " '<',\n",
       " '/textflow',\n",
       " '>',\n",
       " '<',\n",
       " '/applet',\n",
       " '>',\n",
       " \"'\",\n",
       " ')',\n",
       " ';',\n",
       " '}',\n",
       " '}',\n",
       " 'document.write',\n",
       " '(',\n",
       " '``',\n",
       " '<',\n",
       " 'COMMENT',\n",
       " '>',\n",
       " \"''\",\n",
       " ')',\n",
       " ';',\n",
       " '//',\n",
       " '--',\n",
       " '>',\n",
       " 'var',\n",
       " 'si',\n",
       " '=',\n",
       " 'document.location+',\n",
       " \"''\",\n",
       " \"''\",\n",
       " ';',\n",
       " 'var',\n",
       " 'tsi',\n",
       " '=',\n",
       " 'si.replace',\n",
       " '(',\n",
       " '``',\n",
       " '.stm',\n",
       " \"''\",\n",
       " ',',\n",
       " \"''\",\n",
       " \"''\",\n",
       " ')',\n",
       " '.substr',\n",
       " '(',\n",
       " 'si.length-11',\n",
       " ',',\n",
       " 'si.length',\n",
       " ')',\n",
       " ';',\n",
       " 'if',\n",
       " '(',\n",
       " '!',\n",
       " 'tsi.match',\n",
       " '(',\n",
       " '/\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d/',\n",
       " ')',\n",
       " ')',\n",
       " '{',\n",
       " 'tsi',\n",
       " '=',\n",
       " '0',\n",
       " ';',\n",
       " '}',\n",
       " 'document.write',\n",
       " '(',\n",
       " \"'\",\n",
       " '<',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//stats.bbc.co.uk/o.gif',\n",
       " '?',\n",
       " '~RS~s~RS~News~RS~t~RS~HighWeb_Legacy~RS~i~RS~',\n",
       " \"'\",\n",
       " '+',\n",
       " 'tsi',\n",
       " '+',\n",
       " \"'~RS~p~RS~0~RS~u~RS~/2/hi/health/2284783.stm~RS~r~RS~\",\n",
       " '(',\n",
       " 'none',\n",
       " ')',\n",
       " '~RS~a~RS~International~RS~q~RS~~RS~z~RS~46~RS~',\n",
       " \"''\",\n",
       " '>',\n",
       " \"'\",\n",
       " ')',\n",
       " ';']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens\n",
    "# ['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[110:390]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "text.concordance('gene')\n",
    "# Displaying 5 of 5 matches:\n",
    "# hey say too few people now carry the gene for blondes to last beyond the next\n",
    "# blonde hair is caused by a recessive gene . In order for a child to have blond\n",
    "# have blonde hair , it must have the gene on both sides of the family in the g\n",
    "# ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
    "# des would disappear is if having the gene was a disadvantage and I do not thin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Search Engine Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install feedparser\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llog['feed']['title']\n",
    "# 'Language Log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llog.entries)\n",
    "# 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = llog.entries[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emoji in Chinese music video lyric'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.title\n",
    "# \"He's My BF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = post.content[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>From Charles Belov:</p>\\n<p style=\"padding-left: 40px;\">I thought I '"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:70]\n",
    "# '<p>Today I was chatting with three of our visiting graduate students f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = BeautifulSoup(content).get_text()\n",
    "raw = BeautifulSoup(content, \"lxml\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 'Charles',\n",
       " 'Belov',\n",
       " ':',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'I',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sending',\n",
       " 'you',\n",
       " 'a',\n",
       " 'case',\n",
       " 'of',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " 'munging',\n",
       " 'a',\n",
       " 'song',\n",
       " 'lyric',\n",
       " 'when',\n",
       " 'translating',\n",
       " 'it',\n",
       " 'from',\n",
       " 'Chinese',\n",
       " 'to',\n",
       " 'English',\n",
       " '.',\n",
       " 'Instead',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'sending',\n",
       " 'you',\n",
       " 'a',\n",
       " 'case',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Chinese',\n",
       " 'music',\n",
       " 'video',\n",
       " 'making',\n",
       " 'use',\n",
       " 'of',\n",
       " 'an',\n",
       " 'emoji',\n",
       " 'in',\n",
       " 'the',\n",
       " 'song',\n",
       " 'lyrics',\n",
       " '.',\n",
       " 'The',\n",
       " 'song',\n",
       " 'in',\n",
       " 'question',\n",
       " 'is',\n",
       " 'gǎibiàn',\n",
       " '改變',\n",
       " '(',\n",
       " 'Changes',\n",
       " ')',\n",
       " 'by',\n",
       " 'Taiwan',\n",
       " 'rocker',\n",
       " 'Zhāng',\n",
       " 'Zhènyuè',\n",
       " '張震嶽',\n",
       " 'A-Yue',\n",
       " '.',\n",
       " 'If',\n",
       " 'I',\n",
       " 'copy',\n",
       " 'the',\n",
       " 'lyrics',\n",
       " 'from',\n",
       " 'Rock',\n",
       " 'Records',\n",
       " \"'\",\n",
       " 'posting',\n",
       " 'on',\n",
       " 'YouTube',\n",
       " ',',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " 'translates',\n",
       " 'the',\n",
       " 'line',\n",
       " 'in',\n",
       " 'question',\n",
       " '``',\n",
       " 'Wǒ',\n",
       " 'xiǎng',\n",
       " 'dàbiàn',\n",
       " '我想大便',\n",
       " \"''\",\n",
       " 'as',\n",
       " '``',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bowel',\n",
       " 'movement',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Now',\n",
       " 'I',\n",
       " 'am',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'the',\n",
       " 'term',\n",
       " 'dai',\n",
       " 'bihn',\n",
       " '(',\n",
       " 'in',\n",
       " 'Cantonese',\n",
       " ')',\n",
       " 'as',\n",
       " 'to',\n",
       " 'defecate',\n",
       " 'and',\n",
       " 'I',\n",
       " '(',\n",
       " 'mistakenly',\n",
       " ')',\n",
       " 'guessed',\n",
       " 'the',\n",
       " 'phrase',\n",
       " 'must',\n",
       " 'mean',\n",
       " 'something',\n",
       " 'else',\n",
       " 'as',\n",
       " 'well',\n",
       " 'and',\n",
       " 'that',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " 'chose',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'meaning',\n",
       " '.',\n",
       " 'But',\n",
       " ',',\n",
       " 'as',\n",
       " 'I',\n",
       " 're-watched',\n",
       " 'the',\n",
       " 'video',\n",
       " ',',\n",
       " 'I',\n",
       " 'noticed',\n",
       " 'that',\n",
       " 'at',\n",
       " '1:57',\n",
       " 'they',\n",
       " 'gave',\n",
       " 'the',\n",
       " 'lyric',\n",
       " 'as',\n",
       " '``',\n",
       " '我想',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'is',\n",
       " ',',\n",
       " 'using',\n",
       " 'the',\n",
       " 'emoji',\n",
       " 'at',\n",
       " 'U+1F4A9',\n",
       " ',',\n",
       " 'pile',\n",
       " 'of',\n",
       " 'poop',\n",
       " '.',\n",
       " 'So',\n",
       " 'no',\n",
       " 'mis-translation',\n",
       " 'at',\n",
       " 'all',\n",
       " ',',\n",
       " 'only',\n",
       " 'a',\n",
       " 'mismatch',\n",
       " 'in',\n",
       " 'register',\n",
       " '.',\n",
       " 'For',\n",
       " 'those',\n",
       " 'who',\n",
       " 'are',\n",
       " 'interested',\n",
       " ',',\n",
       " 'the',\n",
       " 'lyrics',\n",
       " 'are',\n",
       " 'available',\n",
       " 'on',\n",
       " 'the',\n",
       " 'YouTube',\n",
       " 'site',\n",
       " 'for',\n",
       " 'the',\n",
       " 'song',\n",
       " ',',\n",
       " 'and',\n",
       " 'if',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'mean',\n",
       " ',',\n",
       " 'you',\n",
       " 'can',\n",
       " 'copy',\n",
       " 'and',\n",
       " 'paste',\n",
       " 'them',\n",
       " 'into',\n",
       " 'Google',\n",
       " 'Translate',\n",
       " ',',\n",
       " 'which',\n",
       " 'gives',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'English',\n",
       " 'rendering',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'A',\n",
       " 'point',\n",
       " 'worth',\n",
       " 'noting',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'title',\n",
       " 'and',\n",
       " 'theme',\n",
       " 'of',\n",
       " 'the',\n",
       " 'song',\n",
       " ',',\n",
       " 'gǎibiàn',\n",
       " '改變',\n",
       " '(',\n",
       " '``',\n",
       " 'change',\n",
       " \"''\",\n",
       " ')',\n",
       " ',',\n",
       " 'is',\n",
       " 'the',\n",
       " 'disyllabic',\n",
       " 'form',\n",
       " 'of',\n",
       " 'biàn',\n",
       " '變',\n",
       " '(',\n",
       " '``',\n",
       " 'change',\n",
       " ';',\n",
       " 'transform',\n",
       " \"''\",\n",
       " ')',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'homophonous',\n",
       " 'with',\n",
       " 'the',\n",
       " '``',\n",
       " 'biàn',\n",
       " '便',\n",
       " \"''\",\n",
       " '(',\n",
       " '``',\n",
       " 'convenience',\n",
       " \"''\",\n",
       " ')',\n",
       " 'of',\n",
       " '``',\n",
       " 'dàbiàn',\n",
       " '大便',\n",
       " '(',\n",
       " '``',\n",
       " 'greater',\n",
       " 'convenience',\n",
       " \"''\",\n",
       " '[',\n",
       " '#',\n",
       " '2',\n",
       " ']',\n",
       " ')',\n",
       " 'and',\n",
       " '``',\n",
       " 'xiǎobiàn',\n",
       " '小便',\n",
       " \"''\",\n",
       " '(',\n",
       " '``',\n",
       " 'lesser',\n",
       " 'convenience',\n",
       " \"''\",\n",
       " '[',\n",
       " '#',\n",
       " '1',\n",
       " ']',\n",
       " ')',\n",
       " '.',\n",
       " 'For',\n",
       " 'the',\n",
       " 'ultimate',\n",
       " 'Buddhist',\n",
       " 'derivation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'expressions',\n",
       " ',',\n",
       " 'see',\n",
       " 'the',\n",
       " 'Selected',\n",
       " 'readings',\n",
       " '.',\n",
       " 'Selected',\n",
       " 'readings',\n",
       " \"''\",\n",
       " 'Linguistic',\n",
       " 'advice',\n",
       " 'in',\n",
       " 'the',\n",
       " 'lavatory',\n",
       " ':',\n",
       " 'speaking',\n",
       " 'Mandarin',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'convenience',\n",
       " 'for',\n",
       " 'everyone',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/11/07',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Just',\n",
       " 'the',\n",
       " 'Queen',\n",
       " 'invites',\n",
       " 'irrigation',\n",
       " \"''\",\n",
       " '(',\n",
       " '4/8/08',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Chinese',\n",
       " 'lesson',\n",
       " 'for',\n",
       " 'today',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/29/10',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Next',\n",
       " 'day',\n",
       " \"'s\",\n",
       " 'Chinese',\n",
       " 'lesson',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/31/10',\n",
       " ')',\n",
       " \"''\",\n",
       " 'It',\n",
       " 'is',\n",
       " 'forbidden',\n",
       " 'to',\n",
       " 'urinate',\n",
       " 'here',\n",
       " '.',\n",
       " 'The',\n",
       " 'penalty',\n",
       " 'is',\n",
       " 'bang',\n",
       " '.',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/2/10',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Urination',\n",
       " 'is',\n",
       " 'inhuman',\n",
       " \"''\",\n",
       " '(',\n",
       " '2/6/11',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Do',\n",
       " \"n't\",\n",
       " 'pee',\n",
       " 'on',\n",
       " 'this',\n",
       " 'teapot',\n",
       " \"''\",\n",
       " '(',\n",
       " '3/28/13',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Greater',\n",
       " 'and',\n",
       " 'lesser',\n",
       " 'conveniences',\n",
       " \"''\",\n",
       " '(',\n",
       " '6/25/14',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Sino-Sanskritic',\n",
       " \"'devil\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " '(',\n",
       " '12/11/18',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Toilet',\n",
       " 'Revolution',\n",
       " '!',\n",
       " '!',\n",
       " \"''\",\n",
       " '(',\n",
       " '11/26/17',\n",
       " ')',\n",
       " '—',\n",
       " 'see',\n",
       " 'particularly',\n",
       " 'the',\n",
       " 'last',\n",
       " 'comment',\n",
       " 'for',\n",
       " 'numerous',\n",
       " 'articles',\n",
       " 'on',\n",
       " 'this',\n",
       " 'subject',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Chinese',\n",
       " 'press',\n",
       " \"''\",\n",
       " 'Toilet',\n",
       " 'revolution',\n",
       " '!',\n",
       " ',',\n",
       " 'part',\n",
       " '2',\n",
       " \"''\",\n",
       " '(',\n",
       " '11/22/18',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Toilet',\n",
       " 'revolution',\n",
       " ',',\n",
       " 'an',\n",
       " 'unfinished',\n",
       " 'business',\n",
       " ':',\n",
       " 'beware',\n",
       " '!',\n",
       " \"''\",\n",
       " '(',\n",
       " '5/25/19',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Civilized',\n",
       " 'urinating',\n",
       " \"''\",\n",
       " '(',\n",
       " '10/31/17',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Just',\n",
       " 'the',\n",
       " 'Queen',\n",
       " 'invites',\n",
       " 'irrigation',\n",
       " \"''\",\n",
       " '(',\n",
       " '4/8/08',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Chinese',\n",
       " 'lesson',\n",
       " 'for',\n",
       " 'today',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/29/10',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Next',\n",
       " 'day',\n",
       " \"'s\",\n",
       " 'Chinese',\n",
       " 'lesson',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/31/10',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Urination',\n",
       " 'is',\n",
       " 'inhuman',\n",
       " \"''\",\n",
       " '(',\n",
       " '2/6/11',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Signs',\n",
       " 'from',\n",
       " 'Kashgar',\n",
       " 'to',\n",
       " 'Delhi',\n",
       " \"''\",\n",
       " '(',\n",
       " '10/11/13',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Please',\n",
       " 'pee',\n",
       " 'in',\n",
       " 'the',\n",
       " 'pool',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/4/14',\n",
       " ')',\n",
       " \"''\",\n",
       " \"'Please\",\n",
       " 'enter',\n",
       " 'your',\n",
       " 'cock',\n",
       " 'after',\n",
       " 'urinating',\n",
       " \"'\",\n",
       " \"''\",\n",
       " '(',\n",
       " '4/9/16',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Bovine',\n",
       " '/',\n",
       " 'friggin',\n",
       " \"'\",\n",
       " 'toilet',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/6/18',\n",
       " ')']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(raw)\n",
    "# ['Today', 'I', 'was', 'chatting', 'with', 'three', 'of', 'our', 'visiting',\n",
    "# 'graduate', 'students', 'from', 'the', 'PRC', '.', 'Thinking', 'that', 'I',\n",
    "# 'was', 'being', 'au', 'courant', ',', 'I', 'mentioned', 'the', 'expression',\n",
    "# 'DUI4XIANG4', '\\u5c0d\\u8c61', '(\"', 'boy', '/', 'girl', 'friend', '\"', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('document.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello\\nWorld!\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " 'document.txt',\n",
       " 'groucho_grammar.cfg',\n",
       " 'iu_mien_samp.xml',\n",
       " 'mygrammar.cfg',\n",
       " 'nltk-book-chap-x.ipynb',\n",
       " 'nltk-book-chap01.ipynb',\n",
       " 'nltk-book-chap02-1.ipynb',\n",
       " 'nltk-book-chap02-2.ipynb',\n",
       " 'nltk-book-chap02-3.ipynb',\n",
       " 'nltk-book-chap02-4.ipynb',\n",
       " 'nltk-book-chap02-5.ipynb',\n",
       " 'nltk-book-chap03-1.ipynb',\n",
       " 'nltk-book-chap03-2.ipynb',\n",
       " 'nltk-book-chap03-3.ipynb',\n",
       " 'nltk-book-chap03-4.ipynb',\n",
       " 'nltk-book-chap04-1.ipynb',\n",
       " 'nltk-book-chap04-2.ipynb',\n",
       " 'nltk-book-chap05-1.ipynb',\n",
       " 'nltk-book-chap05-2.ipynb',\n",
       " 'nltk-book-chap05-3.ipynb',\n",
       " 'nltk-book-chap05-4.ipynb',\n",
       " 'nltk-book-chap05-5.ipynb',\n",
       " 'nltk-book-chap05-6.ipynb',\n",
       " 'nltk-book-chap06-1.ipynb',\n",
       " 'nltk-book-chap06-2.ipynb',\n",
       " 'nltk-book-chap06-3.ipynb',\n",
       " 'nltk-book-chap06-4_5.ipynb',\n",
       " 'nltk-book-chap07-1.ipynb',\n",
       " 'nltk-book-chap07-2.ipynb',\n",
       " 'nltk-book-chap07-3.ipynb',\n",
       " 'nltk-book-chap07-4.ipynb',\n",
       " 'nltk-book-chap07-5.ipynb',\n",
       " 'nltk-book-chap07-6.ipynb',\n",
       " 'nltk-book-chap08-1_2.ipynb',\n",
       " 'nltk-book-chap08-3_4.ipynb',\n",
       " 'nltk-book-chap08-5.ipynb',\n",
       " 'nltk-book-chap08-6.ipynb',\n",
       " 'nltk-book-chap09-1.ipynb',\n",
       " 'nltk-book-chap09-2.ipynb',\n",
       " 'nltk-book-chap09-3.ipynb',\n",
       " 'nltk-book-chap10-1.ipynb',\n",
       " 'nltk-book-chap10-2.ipynb',\n",
       " 'nltk-book-chap10-3.ipynb',\n",
       " 'nltk-book-chap10-4.ipynb',\n",
       " 'nltk-book-chap10-5.ipynb',\n",
       " 'nltk-book-chap11-1.ipynb',\n",
       " 'nltk-book-chap11-2.ipynb',\n",
       " 'nltk-book-chap11-3.ipynb',\n",
       " 'nltk-book-chap11-4_5.ipynb',\n",
       " 'README.md',\n",
       " 't2.pkl']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World!\n"
     ]
    }
   ],
   "source": [
    "f = open('document.txt', 'r')\n",
    "for line in f:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "# raw = open(path, 'rU').read()\n",
    "# /Users/hisakato/Documents/anaconda5/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: \n",
    "# DeprecationWarning: 'U' mode is deprecated\n",
    "raw = open(path, 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text from PDF, MSWord, and Other Binary Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third-party libraries such as pypdf and pywin32 provide access to these formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text: On an exceptionally hot evening early in July\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "# Enter some text: On an exceptionally hot evening early in July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typed 8 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")\n",
    "# You typed 8 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open('document.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)\n",
    "# <class 'str'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)\n",
    "# <class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)\n",
    "# <class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)\n",
    "# <class 'list'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('blog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-35781a6509b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'blog'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Traceback (most recent call last):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#   File \"<stdin>\", line 1, in <module>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# AttributeError: 'str' object has no attribute 'append'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "raw.append('blog')\n",
    "# Traceback (most recent call last):\n",
    "#   File \"<stdin>\", line 1, in <module>\n",
    "# AttributeError: 'str' object has no attribute 'append'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who knows?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles = ['john', 'paul', 'george', 'ringo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-7e82cc265233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mquery\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeatles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Traceback (most recent call last):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#   File \"<stdin>\", line 1, in <module>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# TypeError: cannot concatenate 'str' and 'list' objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": [
    "query + beatles\n",
    "# Traceback (most recent call last):\n",
    "#   File \"<stdin>\", line 1, in <module>\n",
    "# TypeError: cannot concatenate 'str' and 'list' objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
