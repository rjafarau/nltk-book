{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP(01):  Advanced Data Science\n",
    "**(Module 03: Natural Language Processing)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Session 10 - Managing Linguistic Data\n",
    "\n",
    "### Corpus Structure: A Case Study\n",
    "#### The Structure of TIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a sample from the TIMIT Corpus. You can access its documentation in the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids() to see a list of the 160 recorded utterances in the corpus sample.Each item has a phonetic transcription which can be accessed using the phones() method. We can access the corresponding word tokens in the customary way. Both access\n",
    "methods permit an optional argument offset=True, which includes the start and end offsets of the corresponding span in the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phonetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.corpus.timit.word_times('dr1-fvmh0/sa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timitdict = nltk.corpus.timit.transcription_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timitdict['greasy'] + timitdict['wash'] + timitdict['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.corpus.timit.spkrinfo('dr1-fvmh0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Life Cycle of a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also measure the agreement between two independent segmentations of language input, e.g., for tokenization, sentence segmentation, and named entity recognition.Windowdiff is a simple algorithm for evaluating the agreement of two segmentations by running a sliding window over the data and awarding partial credit for near misses. If we preprocess our tokens into a sequence of zeros and ones, to record when a token is followed by a boundary, we can represent the segmentations as strings and apply the windowdiff scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"00000010000000001000000\"\n",
    "s2 = \"00000001000000010000000\"\n",
    "s3 = \"00010000000000000001000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.windowdiff(s1, s1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.windowdiff(s1, s2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.windowdiff(s2, s3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data from Word Processor Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following fragment of a lexical entry: “sleep [sli:p] v.i. condition of body and mind...”. We can key in such text using MSWord, then “Save as Web Page,” then inspect the resulting HTML file:\n",
    "\n",
    "<p class=MsoNormal>sleep\n",
    "<span style='mso-spacerun:yes'> </span>\n",
    "[<span class=SpellE>sli:p</span>]\n",
    "<span style='mso-spacerun:yes'> </span>\n",
    "<b><span style='font-size:11.0pt'>v.i.</span></b>\n",
    "<span style='mso-spacerun:yes'> </span>\n",
    "<i>a condition of body and mind ...<o:p></o:p></i>\n",
    "</p>\n",
    "\n",
    "Observe that the entry is represented as an HTML paragraph, using the <p> element, and that the part of speech appears inside a <span style='font-size:11.0pt'> element. The following program defines the set of legal parts-of-speech, legal_pos. Then it extracts all 11-point content from the dict.htm file and stores it in the set used_pos. Observe that the search pattern contains a parenthesized sub-expression; only the material that matches this subexpression is returned by re.findall. Finally, the program constructs the set of illegal parts-of-speech as the set difference between used_pos and legal_pos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document = open(\"dict.htm\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "used_pos = set(re.findall(pattern, document))\n",
    "illegal_pos = used_pos.difference(legal_pos)\n",
    "print(list(illegal_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Converting HTML created by Microsoft Word into comma-separated values\n",
    "def lexical_data(html_file):\n",
    "    SEP = '_ENTRY'\n",
    "    html = open(html_file).read()\n",
    "    html = re.sub(r'<p', SEP + '<p', html)\n",
    "    text = nltk.clean_html(html)\n",
    "    text = ' '.join(text.split())\n",
    "    for entry in text.split(SEP):\n",
    "        if entry.count(' ') > 2:\n",
    "            yield entry.split(' ', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "writer = csv.writer(open(\"dict1.csv\", \"wb\"))\n",
    "writer.writerows(lexical_data(\"dict.htm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data from Spreadsheets and Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spreadsheets are often used for acquiring wordlists or paradigms. For example, a comparative wordlist may be created using a spreadsheet, with a row for each cognate set and a column for each language (see nltk.corpus.swadesh and www.rosettaproject.org). Most spreadsheet software can export their data in CSV format. As we will see later, it is easy for Python programs to access these using the csv module.\n",
    "Nevertheless, when our goal is simply to extract the contents from a database, it is enough to dump out the tables (or SQL query results) in CSV format and load them into our program. Our program might perform a linguistically motivated query that\n",
    "cannot easily be expressed in SQL, e.g., select all words that appear in example sentences for which no dictionary entry is provided. For this task, we would need to extract enough information from a record for it to be uniquely identified, along with the headwords and example sentences. Let’s suppose this information was now available in a CSV file dict.csv:\n",
    "Now we can express this query as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "lexicon = csv.reader(open('dict.csv'))\n",
    "pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\n",
    "lexemes, defns = zip(*pairs)\n",
    "defn_words = set(w for defn in defns for w in defn.split())\n",
    "sorted(defn_words.difference(lexemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case, the input and output formats are isomorphic. For instance, we might be converting lexical data from Toolbox format to XML, and it is straightforward to transliterate the entries one at a time. The structure of the data is reflected in the structure of the required program: a for loop whose body takes care of a single entry.\n",
    "In another common case, the output is a digested form of the input, such as an inverted file index. Here it is necessary to build an index structure in memory, then write it to a file in the desired format. The following example constructs an index\n",
    "that maps the words of a dictionary definition to the corresponding lexeme for each lexical entry , having tokenized the definition text , and discarded short words. Once the index has been constructed, we open a file and then iterate over the index entries, to write out the lines in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = nltk.Index((defn_word, lexeme)\n",
    "                 for (lexeme, defn) in pairs\n",
    "                 for defn_word in nltk.word_tokenize(defn)\n",
    "                 if len(defn_word) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_file = open(\"dict.idx\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in sorted(idx):\n",
    "    idx_words = ', '.join(idx[word])\n",
    "    idx_line = \"%s: %s\\n\" % (word, idx_words)\n",
    "    idx_file.write(idx_line)\n",
    "idx_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ElementTree Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s ElementTree module provides a convenient way to access data stored in XML files. ElementTree is part of Python’s standard library (since Python 2.5), and is also provided as part of NLTK in case you are using Python 2.4.\n",
    "We will illustrate the use of ElementTree using a collection of Shakespeare plays that have been formatted using XML. Let’s load the XML file and inspect the raw data, first at the top of the file , where we see some XML headers and the name of a schema called play.dtd, followed by the root element PLAY. We pick it up again at the start of Act 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
    "raw = open(merchant_file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(raw[0:168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.etree.ElementTree import ElementTree\n",
    "# merchant = ElementTree().parse(merchant_file)\n",
    "from xml.etree import ElementTree as ET\n",
    "merchant = ET.parse(merchant_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merchant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1][54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1][54][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1][54][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1][54][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merchant[-2][1][54][1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can access the entire tree this way, it is more convenient to search for subelements with particular names. Recall that the elements at the top level have several types. We can iterate over just the types we are interested in (such as the acts), using merchant.findall('ACT'). Here’s an example of doing such tag-specific searches at every level of nesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, act in enumerate(merchant.findall('ACT')):\n",
    "    for j, scene in enumerate(act.findall('SCENE')):\n",
    "        for k, speech in enumerate(scene.findall('SPEECH')):\n",
    "            for line in speech.findall('LINE'):\n",
    "                if 'music' in str(line.text):\n",
    "                    print(\"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]\n",
    "speaker_freq = nltk.FreqDist(speaker_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top5 = speaker_freq.keys()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = nltk.defaultdict(lambda: 'OTH')\n",
    "for s in top5:\n",
    "    mapping[s] = s[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speaker_seq2 = [mapping[s] for s in speaker_seq]\n",
    "cfd = nltk.ConditionalFreqDist(nltk.ibigrams(speaker_seq2))\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ElementTree for Accessing Toolbox Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the toolbox.xml() method to access a Toolbox file and load it into an ElementTree object. This file contains a lexicon for the Rotokas language of Papua New Guinea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon[3][0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon[3][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.etree.ElementTree import ElementTree\n",
    "tree = ElementTree(lexicon[3])\n",
    "tree.write(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html = \"<table>\\n\"\n",
    "for entry in lexicon[70:80]:\n",
    "    lx = entry.findtext('lx')\n",
    "    ps = entry.findtext('ps')\n",
    "    ge = entry.findtext('ge')\n",
    "    html += \" <tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (lx, ps, ge)\n",
    "    html += \"</table>\"\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Toolbox Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the popularity of Toolbox among linguists, we will discuss some further methods for working with Toolbox data. Many of the methods discussed in previous chapters, such as counting, building frequency distributions, and tabulating co-occurrences, can be applied to the content of Toolbox entries. For example, we can trivially compute the average number of fields for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "lexicon = toolbox.xml('rotokas.dic')\n",
    "sum(len(entry) for entry in lexicon) / len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Field to Each Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding a new cv field to a lexical entry\n",
    "def cv(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z]', r'_', s)\n",
    "    s = re.sub(r'[aeiou]', r'V', s)\n",
    "    s = re.sub(r'[^V_]', r'C', s)\n",
    "    return (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cv_field(entry):\n",
    "    for field in entry:\n",
    "        if field.tag == 'lx':\n",
    "            cv_field = SubElement(entry, 'cv')\n",
    "            cv_field.text = cv(field.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_cv_field(lexicon[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nltk.to_sfm_string(lexicon[53]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating a Toolbox Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many lexicons in Toolbox format do not conform to any particular schema. Some entries may include extra fields, or may order existing fields in a new way. Manually inspecting thousands of lexical entries is not practicable. However, we can easily identify frequent versus exceptional field sequences, with the help of a FreqDist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(':'.join(field.tag for field in entry) for entry in lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validating Toolbox entries using a context-free grammar.\n",
    "grammar = nltk.parse_cfg('''\n",
    "    S -> Head PS Glosses Comment Date Sem_Field Examples\n",
    "    Head -> Lexeme Root\n",
    "    Lexeme -> \"lx\"\n",
    "    Root -> \"rt\" |\n",
    "    PS -> \"ps\"\n",
    "    Glosses -> Gloss Glosses |\n",
    "    Gloss -> \"ge\" | \"tkp\" | \"eng\"\n",
    "    Date -> \"dt\"\n",
    "    Sem_Field -> \"sf\"\n",
    "    Examples -> Example Ex_Pidgin Ex_English Examples |\n",
    "    Example -> \"ex\"\n",
    "    Ex_Pidgin -> \"xp\"\n",
    "    Ex_English -> \"xe\"\n",
    "    Comment -> \"cmt\" | \"nt\" |\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_lexicon(grammar, lexicon, ignored_tags):\n",
    "    rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "    for entry in lexicon:\n",
    "        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\n",
    "        if rd_parser.nbest_parse(marker_list):\n",
    "            print(\"+\", ':'.join(marker_list))\n",
    "        else:\n",
    "            print(\"-\", ':'.join(marker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignored_tags = ['arg', 'dcsv', 'pt', 'vx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_lexicon(grammar, lexicon, ignored_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Chunking a Toolbox lexicon: A chunk grammar describing the structure of entries for a lexicon for Iu Mien, a language of China.\n",
    "from nltk_contrib import toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    lexfunc: {<lf>(<lv><ln|le>*)*}\n",
    "    example: {<rf|xv><xn|xe>*}\n",
    "    sense: {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\n",
    "    record: {<lx><hm><sense>+<dt>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.etree.ElementTree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = toolbox.ToolboxData()\n",
    "db.open(nltk.data.find('corpora/toolbox/iu_mien_samp.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = db.parse(grammar, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toolbox.data.indent(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ElementTree(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = open(\"iu_mien_samp.xml\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.write(output, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
